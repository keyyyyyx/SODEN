{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f61ee42-9bf1-475e-9c6d-6639d5d915cd",
   "metadata": {},
   "source": [
    "## Test torch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0db9e84-8a6f-4fce-bf33-33d88fa21a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysoden\n"
     ]
    }
   ],
   "source": [
    "# check current working env\n",
    "import os\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76ee8fc2-fb84-45ae-a406-1a44fdc45d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ffc7c0-d141-47de-931b-9294d9c1b4b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6d07e94-02d0-4ec7-8d3b-e584ac1adb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view: Returns a new tensor with the same data as the self tensor but of a different shape.\n",
    "Lambda_t = y.index_select(-1, torch.tensor([0])).view(-1, 1) ## retrieve Lambda_t from y, returns as a 2-D tensor of 1 element\n",
    "Lambda_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60dd881d-ad3b-40c6-81a4-6591bcb6af45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = y.index_select(-1, torch.tensor([1])).view(-1, 1)  ## retrieve the final time step T from y, returns as a 2-D tensor of 1 element\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "087a3b64-03e0-4ca1-8390-dcb1d2d0fa4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5, 6])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = y.index_select(-1, torch.tensor(range(2, y.size(-1))))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbbf0038-ace7-458b-831d-1521d127826c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c9/0gy2v_0d78n1jtsyrj6hc4qm0000gn/T/ipykernel_39378/3381573197.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(x, dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5, 6])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(x, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d3a42c7-b513-4f0b-a258-af0a56839cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5, 6])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9fcec90-1d04-48f2-81e2-377366a10498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "feature_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b15fedda-cb10-4822-8a04-57f961509d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4078, -0.7414, -1.5151, -0.7076],\n",
       "        [-0.2540,  0.5423, -0.4350,  1.1568],\n",
       "        [-0.7054, -0.3997,  0.3515, -0.0304],\n",
       "        [ 2.1536,  0.7084, -0.3678, -1.0363]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = nn.Embedding(1602, feature_size)\n",
    "embed(x.to(torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38dfae83-22fb-4f47-bddd-c48558ce7310",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor(5)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6725fa31-eb60-4231-bf8e-3df74ff031db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 10,  3,  4,  5,  6]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate Lambda_t, t*T, and x into a 1-D tensor\n",
    "inp = torch.cat(\n",
    "            [Lambda_t,\n",
    "             t.repeat(T.size()) * T,  # s = t * T\n",
    "             x.view(-1, feature_size)], dim=1)\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "44df47fa-dd26-4ff3-aeee-e92ee9a8b1a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "906dcc31-e957-4f3f-8a0d-cab752cef996",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7bfaec6-cbff-4b23-8835-701b7013c101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.repeat(T.size()) # Repeats the tensor t along the specified dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a2ee94a-e779-49ac-a622-65204fad5ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.repeat(T.size()) * T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7d5ec26-798d-45aa-b34a-cf347954adc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "589b9e3e-c622-44d2-98f5-d96af67e9577",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5, 6])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "89fa6eb9-7c1d-447e-91fb-f9fbd23e6850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## make a complete nn model with specified layers and sizes\n",
    "def make_net(input_size, hidden_size, num_layers, output_size, dropout=0,\n",
    "             batch_norm=False, act=\"relu\", softplus=True):\n",
    "    if act == \"selu\":\n",
    "        ActFn = nn.SELU\n",
    "    else:\n",
    "        ActFn = nn.ReLU\n",
    "    modules = [nn.Linear(input_size, hidden_size), ActFn()]   ## Applies a linear transformation to the incoming data\n",
    "    if batch_norm:\n",
    "        modules.append(nn.BatchNorm1d(hidden_size))\n",
    "    if dropout > 0:\n",
    "        modules.append(nn.Dropout(p=dropout))\n",
    "    if num_layers > 1:\n",
    "        for _ in range(num_layers - 1):\n",
    "            modules.append(nn.Linear(hidden_size, hidden_size))\n",
    "            modules.append(ActFn())\n",
    "            if batch_norm:\n",
    "                modules.append(nn.BatchNorm1d(hidden_size))\n",
    "            if dropout > 0:\n",
    "                modules.append(nn.Dropout(p=dropout))\n",
    "    modules.append(nn.Linear(hidden_size, output_size))\n",
    "    if softplus:  # ODE models\n",
    "        modules.append(nn.Softplus())\n",
    "    return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf795c90-1e93-4e53-8bb7-6496fcd86204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = make_net(6, 3, num_layers = 2, output_size = 1, dropout=0,\n",
    "             batch_norm=False, act=\"relu\", softplus=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af77239f-43d1-4434-9217-206e5dad07f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# solve dtype error\n",
    "inp.dtype\n",
    "inp = inp.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ce4e2130-40cd-419c-9f6c-88c2162975d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4979]], grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e872a6cf-d467-4eb3-9376-d6b02c944372",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9958]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(inp) * T\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "caaefc67-76aa-4a91-b266-6e113d35f5e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f36ec-0c08-47ba-bc3a-ecbfe3f28aca",
   "metadata": {},
   "source": [
    "## Test foward function in NonCoxFuncModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "507c98f3-9ddf-4a46-9d5f-6f86c8415340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a sample input\n",
    "inputs = {\n",
    "  \"t\": torch.tensor(5),\n",
    "  \"init_cond\": torch.tensor(1),\n",
    "  \"features\": torch.tensor([[3, 4, 5, 6]])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5cbd4bba-fa66-409c-bcf9-c79431a0742a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:  tensor(5)\n",
      "init_cond:  tensor(1)\n",
      "features:  tensor([[3, 4, 5, 6]])\n",
      "new t:  tensor([0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# retrieve information from input\n",
    "t = inputs[\"t\"]\n",
    "print(\"t: \", t)\n",
    "init_cond = inputs[\"init_cond\"]\n",
    "print(\"init_cond: \", init_cond)\n",
    "features = inputs[\"features\"]\n",
    "print(\"features: \", features)\n",
    "init_cond = torch.cat([init_cond.view(-1, 1), t.view(-1, 1), features], dim=1)  ## rearrange; equiv to c(init_cond, t, features)\n",
    "t = torch.tensor([0., 1.])\n",
    "print(\"new t: \", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "434fdb69-e40e-4a2e-901d-c8cd1dd3a513",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_cond.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2ecde82b-0c07-4506-97a8-94138fa86472",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bf694bde-c468-488f-b89e-238ba642cced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 5, 3, 4, 5, 6]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6500e835-6946-4153-baf4-c3606ebb7600",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseSurvODEFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseSurvODEFunc, self).__init__()\n",
    "        self.nfe = 0\n",
    "        self.batch_time_mode = False\n",
    "\n",
    "    def set_batch_time_mode(self, mode=True):\n",
    "        self.batch_time_mode = mode\n",
    "        # `odeint` requires the output of `odefunc` to have the same size as\n",
    "        # `init_cond` despite the how many steps we are going to evaluate. Set\n",
    "        # `self.batch_time_mode` to `False` before calling `odeint`. However,\n",
    "        # when we want to call the forward function of `odefunc` directly and\n",
    "        # when we would like to evaluate multiple time steps at the same time,\n",
    "        # set `self.batch_time_mode` to `True` and the output will have size\n",
    "        # (len(t), size(y)).\n",
    "\n",
    "    ## What is nfe??\n",
    "    def reset_nfe(self):\n",
    "        self.nfe = 0\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        raise NotImplementedError(\"Not implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "858debe2-8527-440f-8a0a-4c073393609a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContextRecMLPODEFunc(BaseSurvODEFunc):\n",
    "    def __init__(self, feature_size, hidden_size, num_layers, batch_norm=False,\n",
    "                 use_embed=True):\n",
    "        super(ContextRecMLPODEFunc, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_norm = batch_norm\n",
    "        self.use_embed = use_embed\n",
    "        if use_embed:\n",
    "            self.embed = nn.Embedding(1602, self.feature_size)  ## A simple lookup table that maps index value to a weighted matrix of certain dimension\n",
    "        else:\n",
    "            self.embed = None\n",
    "        self.net = make_net(input_size=feature_size+2, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, output_size=1,\n",
    "                            batch_norm=batch_norm)\n",
    "    \n",
    "    ## ---where does the y come from?--- passed within odeint as init_cond?\n",
    "    ## forward propagetion; one step forward\n",
    "    ## outputs a number from nn\n",
    "    def forward(self, t, y):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "          t: When self.batch_time_mode is False, t is a scalar indicating the\n",
    "            time step to be evaluated. When self.batch_time_mode is True, t is\n",
    "            a 1-D tensor with a single element [1.0].\n",
    "          y: When self.batch_time_mode is False, y is a 1-D tensor with length\n",
    "            2 + k, where the first dim indicates Lambda_t, the second dim\n",
    "            indicates the final time step T to be evaluated, and the remaining\n",
    "            k dims indicates the features. When self.batch_time_mode is True, y\n",
    "            is a 2-D tensor with batch_size * (2 + k).\n",
    "        \"\"\"\n",
    "        self.nfe += 1\n",
    "        device = next(self.parameters()).device\n",
    "        Lambda_t = y.index_select(-1, torch.tensor([0]).to(device)).view(-1, 1) ## retrieve Lambda_t from y, returns as a 2-D tensor of 1 element\n",
    "        T = y.index_select(-1, torch.tensor([1]).to(device)).view(-1, 1)  ## retrieve the final time step T from y, returns as a 2-D tensor of 1 element\n",
    "        x = y.index_select(-1, torch.tensor(range(2, y.size(-1))).to(device))  ## retrieve features from y, returns as a 1-D tensor\n",
    "        if self.use_embed:\n",
    "            x = torch.mean(\n",
    "                self.embed(torch.tensor(x, dtype=torch.long).to(device)),\n",
    "                dim=1)\n",
    "        # Rescaling trick  ## time rescaling\n",
    "        # $\\int_0^T f(s; x) ds = \\int_0^1 T f(tT; x) dt$, where $t = s / T$\n",
    "        inp = torch.cat(\n",
    "            [Lambda_t,\n",
    "             t.repeat(T.size()) * T,  # s = t * T; time step to be evaluated * final time step\n",
    "             x.view(-1, self.feature_size)], dim=1)\n",
    "        output = self.net(inp) * T  # f(tT; x) * T\n",
    "        zeros = torch.zeros_like(\n",
    "            y.index_select(-1, torch.tensor(range(1, y.size(-1))).to(device))\n",
    "        )  ## Returns a tensor filled with the scalar value 0, with the same size as input\n",
    "        output = torch.cat([output, zeros], dim=1)\n",
    "        if self.batch_time_mode:\n",
    "            return output\n",
    "        else:\n",
    "            return output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2f82b4fe-92aa-451f-8ec1-9f8f3429158e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define sample config\n",
    "config = {\n",
    "    \"hidden_size\": 5,\n",
    "    \"num_layers\": 2,\n",
    "    \"batch_norm\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0337cafa-b66c-4c96-9602-ae9010e06fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define odefunc\n",
    "odefunc = ContextRecMLPODEFunc(\n",
    "                feature_size = 4, hidden_size = config[\"hidden_size\"], num_layers = config[\"num_layers\"],\n",
    "                batch_norm=config[\"batch_norm\"], use_embed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "added323-a943-4ad4-b088-d79250e99e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initilize output\n",
    "outputs = {}\n",
    "# import ode solver\n",
    "from torchdiffeq import odeint_adjoint as odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "82e894bf-7219-4133-93de-1384fdf85428",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Lambda': tensor([[4.2693, 5.0000, 3.0000, 4.0000, 5.0000, 6.0000]],\n",
      "       grad_fn=<ViewBackward0>), 'lambda': tensor([3.2847, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       grad_fn=<SqueezeBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "# change init_cond to floating point Tensor to fix datatype error\n",
    "init_cond = init_cond.to(torch.float)\n",
    "# solve ode for Lambda\n",
    "outputs[\"Lambda\"] = odeint(odefunc, init_cond, t, rtol=1e-4, atol=1e-8)[1:].squeeze()  # size: [length of t] x [batch size] x [dim of y0]  ## Solve ODE for cumulative hazard function\n",
    "outputs[\"Lambda\"] = outputs[\"Lambda\"].view(1, outputs[\"Lambda\"].size(-1)) ## add to fix dimension error\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c552781f-98d8-47f1-b25f-db0549bf5b21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Lambda': tensor([4.2693], grad_fn=<SelectBackward0>), 'lambda': tensor([0.6569], grad_fn=<DivBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "# add outputs\n",
    "odefunc.set_batch_time_mode(True)\n",
    "outputs[\"lambda\"] = odefunc(t[1:], outputs[\"Lambda\"]).squeeze()\n",
    "outputs[\"lambda\"] = outputs[\"lambda\"].view(1, outputs[\"lambda\"].size(-1)) ## add to fix dimension error\n",
    "outputs[\"Lambda\"] = outputs[\"Lambda\"][:, 0]\n",
    "outputs[\"lambda\"] = outputs[\"lambda\"][:, 0] / inputs[\"t\"]\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d7a25-0f77-4f1f-a266-ea46f8936b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "775f967b-2368-44a6-8377-fe0a8e5893b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debug\n",
    "t[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "99e7b33e-5fea-46c5-8b85-3b9a40774a7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"Lambda\"].dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2e83e3c1-b78d-40e0-9cba-2b999a700070",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLambda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "outputs[\"Lambda\"][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e95cb837-addd-42c4-acfa-3d596878ee11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([[4.2693, 5.0000, 3.0000, 4.0000, 5.0000, 6.0000]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "t_1: tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "# debug dimsion error inside ContextRecMLPODEFunc\n",
    "# convert y to 2d solves the problem\n",
    "y = outputs[\"Lambda\"]\n",
    "y = y.view(1, 6)\n",
    "print(\"y:\", y)\n",
    "t_1 = t[1:]\n",
    "print(\"t_1:\", t_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8f2fe63f-3fb6-4f1f-9f64-b57d2cf1e509",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda_t: tensor([[4.2693]], grad_fn=<ViewBackward0>)\n",
      "T:  tensor([[5.]], grad_fn=<ViewBackward0>)\n",
      "x: tensor([[3., 4., 5., 6.]], grad_fn=<IndexSelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Lambda_t = y.index_select(-1, torch.tensor([0])).view(-1, 1) ## retrieve Lambda_t from y, returns as a 2-D tensor of 1 element\n",
    "T = y.index_select(-1, torch.tensor([1])).view(-1, 1)  ## retrieve the final time step T from y, returns as a 2-D tensor of 1 element\n",
    "x = y.index_select(-1, torch.tensor(range(2, y.size(-1))))\n",
    "print(\"Lambda_t:\", Lambda_t)\n",
    "print(\"T: \", T)\n",
    "print(\"x:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "63b726ad-ca95-4872-bad2-a15276870233",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tensor([[4.2693, 5.0000, 3.0000, 4.0000, 5.0000, 6.0000]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "output: tensor([[2.7066]], grad_fn=<MulBackward0>)\n",
      "zeros: tensor([[0., 0., 0., 0., 0.]])\n",
      "output: tensor([[2.7066, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "feature_size = 4\n",
    "inp = torch.cat([Lambda_t, t_1.repeat(T.size()) * T,  # s = t * T; time step to be evaluated * final time step\n",
    "             x.view(-1, feature_size)], dim=1)\n",
    "print(\"inp:\", inp)\n",
    "output = net(inp) * T  # f(tT; x) * T\n",
    "print(\"output:\", output)\n",
    "zeros = torch.zeros_like(y.index_select(-1, torch.tensor(range(1, y.size(-1)))))  ## Returns a tensor filled with the scalar value 0, with the same size as input\n",
    "print(\"zeros:\", zeros)\n",
    "output = torch.cat([output, zeros], dim=1)\n",
    "print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8fa293e5-687f-4448-9efd-7703443b21de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(t_1.repeat(T.size()) * T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "81ae0866-a6a9-4701-8197-f3a12acc6d44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0db4e834-6c71-4b93-9524-1a71539fe1eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e59321d5-60ad-46b1-a22c-8af88ac440fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea929e5-9531-478e-b6b0-4fb2d43625c6",
   "metadata": {},
   "source": [
    "## Test SODENModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b45e7325-b020-42f7-8dc4-de144982cfc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# store NonCoxFuncModel\n",
    "## The proposed SODEN model\n",
    "class NonCoxFuncModel(nn.Module):\n",
    "    \"\"\"NonCoxFuncModel.\"\"\"\n",
    "\n",
    "    def __init__(self, model_config, feature_size=None, use_embed=False):\n",
    "        \"\"\"Initializes a NonCoxFuncModel.\n",
    "\n",
    "        Arguments:\n",
    "          model_config: An OrderedDict of lists. The keys of the dict indicate\n",
    "            the names of different parts of the model. Each value of the dict\n",
    "            is a list indicating the configs of layers in the corresponding\n",
    "            part. Each element of the list is a list [layer_type, arguments],\n",
    "            where layer_type is a string and arguments is a dict.\n",
    "          feature_size: Feature size.\n",
    "          use_embed: Whether to use embedding layer after input.\n",
    "        \"\"\"\n",
    "        assert feature_size is not None\n",
    "        super(NonCoxFuncModel, self).__init__()\n",
    "        self.model_config = model_config\n",
    "        self.feature_size = feature_size\n",
    "        config = model_config[\"ode\"][\"surv_ode_0\"]\n",
    "        self.func_type = config[\"func_type\"]\n",
    "\n",
    "        if self.func_type == \"rec_mlp\":\n",
    "            self.odefunc = ContextRecMLPODEFunc(\n",
    "                feature_size, config[\"hidden_size\"], config[\"num_layers\"],\n",
    "                batch_norm=config[\"batch_norm\"], use_embed=use_embed)     ## initialize ContextRecMLPODEFunc; ode function\n",
    "        else:\n",
    "            raise NotImplementedError(\"Function type %s is not supported.\"\n",
    "                                      % self.func_type)\n",
    "\n",
    "        self.set_last_eval(False)\n",
    "\n",
    "    def set_last_eval(self, last_eval=True):\n",
    "        self.last_eval = last_eval\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        device = next(self.parameters()).device\n",
    "        t = inputs[\"t\"]\n",
    "        init_cond = inputs[\"init_cond\"]\n",
    "        features = inputs[\"features\"]\n",
    "        init_cond = torch.cat([init_cond.view(-1, 1), t.view(-1, 1), features],\n",
    "                              dim=1)  ## rearrange; equiv to c(init_cond, t, features)\n",
    "        t = torch.tensor([0., 1.]).to(device)\n",
    "\n",
    "        outputs = {}\n",
    "        self.odefunc.set_batch_time_mode(False)\n",
    "        outputs[\"Lambda\"] = odeint(\n",
    "            self.odefunc, init_cond, t, rtol=1e-4, atol=1e-8)[1:].squeeze()  # size: [length of t] x [batch size] x [dim of y0]  ## Solve ODE for cumulative hazard function\n",
    "        self.odefunc.set_batch_time_mode(True)\n",
    "        outputs[\"lambda\"] = self.odefunc(t[1:], outputs[\"Lambda\"]).squeeze()  ## Solve ODE for hazard function\n",
    "        outputs[\"Lambda\"] = outputs[\"Lambda\"][:, 0]\n",
    "        outputs[\"lambda\"] = outputs[\"lambda\"][:, 0] / inputs[\"t\"]\n",
    "\n",
    "        if not self.training:  ## ---where is self.training defined?---\n",
    "            if self.last_eval and \"eval_t\" in inputs:\n",
    "                self.odefunc.set_batch_time_mode(False)\n",
    "                ones = torch.ones_like(inputs[\"t\"])\n",
    "                # Eval for time-dependent C-index\n",
    "                outputs[\"t\"] = inputs[\"t\"]\n",
    "                outputs[\"eval_t\"] = inputs[\"eval_t\"]\n",
    "                t = inputs[\"eval_t\"][-1] * ones\n",
    "                init_cond = inputs[\"init_cond\"]\n",
    "                features = inputs[\"features\"]\n",
    "                init_cond = torch.cat(\n",
    "                    [init_cond.view(-1, 1), t.view(-1, 1), features],\n",
    "                    dim=1)\n",
    "                t_max = inputs[\"eval_t\"][-1]\n",
    "                t = inputs[\"eval_t\"] / t_max\n",
    "                t = torch.cat([torch.zeros([1]).to(device), t], dim=0)\n",
    "                outputs[\"cum_hazard_seqs\"] = odeint(\n",
    "                    self.odefunc, init_cond, t, rtol=1e-4, atol=1e-8)[1:, :, 0]  \n",
    "\n",
    "                # Eval for Brier Score\n",
    "                t = inputs[\"t_max\"] * ones\n",
    "                init_cond = inputs[\"init_cond\"]\n",
    "                features = inputs[\"features\"]\n",
    "                init_cond = torch.cat(\n",
    "                    [init_cond.view(-1, 1), t.view(-1, 1), features],\n",
    "                    dim=1)\n",
    "                t_min = inputs[\"t_min\"]\n",
    "                t_max = inputs[\"t_max\"]\n",
    "                t = torch.linspace(\n",
    "                    t_min, t_max, NUM_INT_STEPS, dtype=init_cond.dtype,\n",
    "                    device=device)\n",
    "                t = torch.cat([torch.zeros([1]).to(device), t], dim=0)\n",
    "                t = t / t_max\n",
    "                outputs[\"survival_seqs\"] = torch.exp(\n",
    "                    -odeint(self.odefunc, init_cond, t, rtol=1e-4,\n",
    "                            atol=1e-8)[1:, :, 0])\n",
    "\n",
    "                for eps in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "                    t = inputs[\"t_max_{}\".format(eps)] * ones\n",
    "                    init_cond = inputs[\"init_cond\"]\n",
    "                    features = inputs[\"features\"]\n",
    "                    init_cond = torch.cat(\n",
    "                        [init_cond.view(-1, 1), t.view(-1, 1), features],\n",
    "                        dim=1)\n",
    "                    t_min = inputs[\"t_min\"]\n",
    "                    t_max = inputs[\"t_max_{}\".format(eps)]\n",
    "                    t = torch.linspace(\n",
    "                        t_min, t_max, NUM_INT_STEPS, dtype=init_cond.dtype,\n",
    "                        device=device)\n",
    "                    t = torch.cat([torch.zeros([1]).to(device), t], dim=0)\n",
    "                    t = t / t_max\n",
    "                    outputs[\"survival_seqs_{}\".format(eps)] = torch.exp(\n",
    "                        -odeint(self.odefunc, init_cond, t, rtol=1e-4,\n",
    "                                atol=1e-8)[1:, :, 0])\n",
    "            \n",
    "            ## compute Lambda at q25, q50, and q75 \n",
    "            if \"t_q25\" in inputs:\n",
    "                outputs[\"t\"] = inputs[\"t\"]\n",
    "                self.odefunc.set_batch_time_mode(False)\n",
    "                for q in [\"q25\", \"q50\", \"q75\"]:\n",
    "                    t = inputs[\"t_%s\" % q]\n",
    "                    init_cond = inputs[\"init_cond\"]\n",
    "                    features = inputs[\"features\"]\n",
    "                    init_cond = torch.cat(\n",
    "                        [init_cond.view(-1, 1), t.view(-1, 1), features],\n",
    "                        dim=1)\n",
    "                    t = torch.tensor([0., 1.]).to(device)\n",
    "                    outputs[\"Lambda_%s\" % q] = odeint(\n",
    "                        self.odefunc, init_cond, t,\n",
    "                        rtol=1e-4, atol=1e-8)[1:].squeeze()\n",
    "                    outputs[\"Lambda_%s\" % q] = outputs[\"Lambda_%s\" % q][:, 0]\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5d9e88d2-9420-4c19-8559-8239c8bb7c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_type': 'surv_ode', 'func_type': 'rec_mlp', 'hidden_size': 5, 'num_layers': 2, 'batch_norm': False}\n"
     ]
    }
   ],
   "source": [
    "# define sample model config\n",
    "model_config = {\n",
    "    \"ode\": {\n",
    "        \"surv_ode_0\": {\n",
    "            \"layer_type\": \"surv_ode\",\n",
    "            \"func_type\": \"rec_mlp\",\n",
    "            \"hidden_size\": 5,\n",
    "            \"num_layers\": 2,\n",
    "            \"batch_norm\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "print(model_config[\"ode\"][\"surv_ode_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e9199303-0a71-4a0b-9eb4-39d18d5f438b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RNNModel = nn.LSTM\n",
    "feature_size = {\n",
    "    \"seq_feat\": 5,\n",
    "    \"fix_feat\": 4\n",
    "}\n",
    "seq_feat_size = feature_size[\"seq_feat\"]\n",
    "rnn = RNNModel(input_size=seq_feat_size,\n",
    "                                hidden_size=3,\n",
    "                                num_layers=2,\n",
    "                                batch_first=True)\n",
    "feature_size = 3 + feature_size[\"fix_feat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7db4d3dc-81e0-46ee-8b2e-ab5f35f2bf04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = NonCoxFuncModel(model_config, 4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "801fb7ce-0aaf-4ac9-812f-2b388fa11d17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "`y0` must be a floating point Tensor but is a torch.LongTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mysoden/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[129], line 49\u001b[0m, in \u001b[0;36mNonCoxFuncModel.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m outputs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39modefunc\u001b[38;5;241m.\u001b[39mset_batch_time_mode(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 49\u001b[0m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLambda\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43modefunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# size: [length of t] x [batch size] x [dim of y0]  ## Solve ODE for cumulative hazard function\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39modefunc\u001b[38;5;241m.\u001b[39mset_batch_time_mode(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     52\u001b[0m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39modefunc(t[\u001b[38;5;241m1\u001b[39m:], outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLambda\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m## Solve ODE for hazard function\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mysoden/lib/python3.10/site-packages/torchdiffeq/_impl/adjoint.py:192\u001b[0m, in \u001b[0;36modeint_adjoint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol, adjoint_method, adjoint_options, adjoint_params)\u001b[0m\n\u001b[1;32m    188\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn adjoint parameter was passed without requiring gradient. For efficiency this will be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexcluded from the adjoint pass, and will not appear as a tensor in the adjoint norm.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Convert to flattened state.\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m shapes, func, y0, t, rtol, atol, method, options, event_fn, decreasing_time \u001b[38;5;241m=\u001b[39m \u001b[43m_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSOLVERS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Handle the adjoint norm function.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m state_norm \u001b[38;5;241m=\u001b[39m options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mysoden/lib/python3.10/site-packages/torchdiffeq/_impl/misc.py:213\u001b[0m, in \u001b[0;36m_check_inputs\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn, SOLVERS)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m         event_fn \u001b[38;5;241m=\u001b[39m _TupleInputOnlyFunc(event_fn, shapes)\n\u001b[0;32m--> 213\u001b[0m \u001b[43m_assert_floating\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Normalise method and options\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mysoden/lib/python3.10/site-packages/torchdiffeq/_impl/misc.py:106\u001b[0m, in \u001b[0;36m_assert_floating\u001b[0;34m(name, t)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_assert_floating\u001b[39m(name, t):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_floating_point(t):\n\u001b[0;32m--> 106\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` must be a floating point Tensor but is a \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, t\u001b[38;5;241m.\u001b[39mtype()))\n",
      "\u001b[0;31mTypeError\u001b[0m: `y0` must be a floating point Tensor but is a torch.LongTensor"
     ]
    }
   ],
   "source": [
    "model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a39083-2430-4b65-a738-169b17e0fb18",
   "metadata": {},
   "source": [
    "## Test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "49c60751-bd4e-4d36-af9f-24169a8b81dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import (BatchSampler, DataLoader, Dataset, RandomSampler,\n",
    "                              SequentialSampler)\n",
    "from torch.utils.data.dataloader import default_collate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded08491-9182-40f0-adaa-d04a89fcdef2",
   "metadata": {},
   "source": [
    "### get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "99aa2192-220d-4541-952f-00a8a92f7cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 8 2 8 2 7 2 2 0 7]\n"
     ]
    }
   ],
   "source": [
    "t = np.random.randint(0, 10, size = 10)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b02b1b59-dda3-43e6-932b-6066616f2992",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "N = len(t)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c9b52bad-93bf-41b8-b54c-d902cc5eb3b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 2 4 6 7 0 5 9 1 3]\n"
     ]
    }
   ],
   "source": [
    "idx = np.argsort(t)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e113acfd-126d-4177-be30-e4c1ff6215e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 2 2 2 7 7 7 8 8]\n"
     ]
    }
   ],
   "source": [
    "t = t[idx]\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5710d982-bd1a-40d0-842f-7f61772efcf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "delta = np.random.randint(0, 2, size = 10)\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a535fe4f-0952-4c30-a718-4c2e61bc7293",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "delta = delta[idx]\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4a6fd87b-949c-45e3-b9d1-2a8be9441d23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  7 11  8 13  8 12  8 11  8]\n",
      " [ 6  6  1  9  5  1  2  7 12  4]\n",
      " [ 3 11  1  1  7 10  8 10 13  5]\n",
      " [ 8  7  5  9 12 12 12  9  4  1]\n",
      " [ 7  9  8 12 11  6 12 13  6  9]\n",
      " [ 9  6 14 14 12  1  5 14  1  4]\n",
      " [12 11 13  2 12 11 14  1 10 13]\n",
      " [10 11  2 12  8  5 11 12 12  4]\n",
      " [13  7  4  3  9  3  4  7  4  7]\n",
      " [ 1  7  8  8  7  6  2 13  6  3]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randint(1, 15, size = (10, 10))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e5755e86-53cd-43fe-8c97-da5052e762bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "init_cond = np.zeros_like(t)\n",
    "print(init_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "66841546-caf3-4eae-84e5-e34531dbd79b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "labels = torch.tensor(delta, dtype=torch.float)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3343f761-7f29-4d47-9aa5-5e3d98e66734",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': tensor([0., 2., 2., 2., 2., 7., 7., 7., 8., 8.]), 'init_cond': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'features': tensor([[12.,  7., 11.,  8., 13.,  8., 12.,  8., 11.,  8.],\n",
      "        [ 6.,  6.,  1.,  9.,  5.,  1.,  2.,  7., 12.,  4.],\n",
      "        [ 3., 11.,  1.,  1.,  7., 10.,  8., 10., 13.,  5.],\n",
      "        [ 8.,  7.,  5.,  9., 12., 12., 12.,  9.,  4.,  1.],\n",
      "        [ 7.,  9.,  8., 12., 11.,  6., 12., 13.,  6.,  9.],\n",
      "        [ 9.,  6., 14., 14., 12.,  1.,  5., 14.,  1.,  4.],\n",
      "        [12., 11., 13.,  2., 12., 11., 14.,  1., 10., 13.],\n",
      "        [10., 11.,  2., 12.,  8.,  5., 11., 12., 12.,  4.],\n",
      "        [13.,  7.,  4.,  3.,  9.,  3.,  4.,  7.,  4.,  7.],\n",
      "        [ 1.,  7.,  8.,  8.,  7.,  6.,  2., 13.,  6.,  3.]]), 'index': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}\n"
     ]
    }
   ],
   "source": [
    "features = {}\n",
    "features[\"t\"] = torch.tensor(t, dtype=torch.float)\n",
    "features[\"init_cond\"] = torch.tensor(init_cond, dtype=torch.float)\n",
    "features[\"features\"] = torch.tensor(x, dtype=torch.float)\n",
    "features[\"index\"] = torch.arange(N, dtype=torch.long)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d13e780e-23b6-4662-9205-434c2bde427c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DictDataset.\n",
    "class DictDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_features = {}\n",
    "        for key in self.features:\n",
    "            sample_features[key] = self.features[key][idx]\n",
    "        return sample_features, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c895f75b-23ca-490c-9930-1fcd2b1e1439",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.DictDataset object at 0x7f8a7f7fc790>\n"
     ]
    }
   ],
   "source": [
    "dataset = DictDataset(features, labels)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f7c612a7-e788-43d8-836b-840b7ad18428",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Batch random sampler that maintains order within each batch.\n",
    "class OrderedBatchRandomSampler(object):\n",
    "    def __init__(self, n, batch_size, seed=13, drop_last=False):\n",
    "        super(OrderedBatchRandomSampler, self).__init__()\n",
    "        self.n = n\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.drop_last = drop_last\n",
    "        self.random_state = np.random.RandomState(seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return self.n // self.batch_size\n",
    "        else:\n",
    "            return (self.n + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        for idx in self.random_state.permutation(self.n):  ## generate random indices\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield sorted(batch)\n",
    "                batch = []\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield sorted(batch)  ## Return sends a specified value back to its caller whereas Yield can produce a sequence of values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d4eba6e6-6617-4688-8db3-4eb57eeeef89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampler = OrderedBatchRandomSampler(N, batch_size = 2, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "71d7d6b8-856a-4fa6-b05e-aada09607b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_collate_fn = default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e71e06da-e994-40b5-a071-1f90893acc69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f8a7f7fc7f0>\n"
     ]
    }
   ],
   "source": [
    "NUM_WORKERS = 0\n",
    "dataloader = DataLoader(\n",
    "        dataset, batch_sampler=sampler, collate_fn=_collate_fn, pin_memory=True,\n",
    "        num_workers=NUM_WORKERS)\n",
    "print(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "aca4973f-f33b-4d32-87d9-bdf1127c5616",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'t': tensor([2., 7.]), 'init_cond': tensor([0., 0.]), 'features': tensor([[ 8.,  7.,  5.,  9., 12., 12., 12.,  9.,  4.,  1.],\n",
      "        [ 9.,  6., 14., 14., 12.,  1.,  5., 14.,  1.,  4.]]), 'index': tensor([3, 5])}, tensor([1., 0.])]\n",
      "[{'t': tensor([2., 7.]), 'init_cond': tensor([0., 0.]), 'features': tensor([[ 6.,  6.,  1.,  9.,  5.,  1.,  2.,  7., 12.,  4.],\n",
      "        [12., 11., 13.,  2., 12., 11., 14.,  1., 10., 13.]]), 'index': tensor([1, 6])}, tensor([1., 0.])]\n",
      "[{'t': tensor([2., 7.]), 'init_cond': tensor([0., 0.]), 'features': tensor([[ 7.,  9.,  8., 12., 11.,  6., 12., 13.,  6.,  9.],\n",
      "        [10., 11.,  2., 12.,  8.,  5., 11., 12., 12.,  4.]]), 'index': tensor([4, 7])}, tensor([0., 0.])]\n",
      "[{'t': tensor([8., 8.]), 'init_cond': tensor([0., 0.]), 'features': tensor([[13.,  7.,  4.,  3.,  9.,  3.,  4.,  7.,  4.,  7.],\n",
      "        [ 1.,  7.,  8.,  8.,  7.,  6.,  2., 13.,  6.,  3.]]), 'index': tensor([8, 9])}, tensor([0., 0.])]\n",
      "[{'t': tensor([0., 2.]), 'init_cond': tensor([0., 0.]), 'features': tensor([[12.,  7., 11.,  8., 13.,  8., 12.,  8., 11.,  8.],\n",
      "        [ 3., 11.,  1.,  1.,  7., 10.,  8., 10., 13.,  5.]]), 'index': tensor([0, 2])}, tensor([0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "for batch_data in dataloader:\n",
    "    print(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d5c04f35-5d3b-41cf-a17b-5dbbdbd5f05d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr_0\n",
      "[[ 0.5778179  -0.59537214  0.13440911 ... -0.18676585 -1.3885338\n",
      "   2.489657  ]\n",
      " [ 1.1923819  -1.5297642   0.13440911 ... -0.18676585 -1.3885338\n",
      "   2.489657  ]\n",
      " [-0.6891113  -1.0625682   2.0835433  ... -0.18676585 -1.3885338\n",
      "   2.489657  ]\n",
      " ...\n",
      " [-0.9417017  -0.05629976 -0.8087203  ... -0.18676585  0.72018415\n",
      "  -0.40166175]\n",
      " [ 1.1944975   0.9499686   0.3859103  ... -0.18676585  0.72018415\n",
      "  -0.40166175]\n",
      " [ 0.83124214  0.8780922   0.3859103  ... -0.18676585 -1.3885338\n",
      "   2.489657  ]]\n",
      "(1727, 27)\n",
      "arr_1\n",
      "[[4.18891191 0.        ]\n",
      " [0.05201916 1.        ]\n",
      " [0.75017112 1.        ]\n",
      " ...\n",
      " [1.66461325 0.        ]\n",
      " [0.09856263 1.        ]\n",
      " [0.02737851 1.        ]]\n",
      "(1727, 2)\n",
      "arr_2\n",
      "[[ 71.45795  68.      102.      ...   0.        0.        1.     ]\n",
      " [ 81.03198  42.      102.      ...   0.        0.        1.     ]\n",
      " [ 51.72098  55.      164.      ...   0.        0.        1.     ]\n",
      " ...\n",
      " [ 47.78598  83.       72.      ...   0.        1.        0.     ]\n",
      " [ 81.06494 111.      110.      ...   0.        1.        0.     ]\n",
      " [ 75.40594 109.      110.      ...   0.        0.        1.     ]]\n",
      "(1727, 27)\n"
     ]
    }
   ],
   "source": [
    "from numpy import load\n",
    "\n",
    "data = load('data/support/test_1.npz')\n",
    "lst = data.files\n",
    "for item in lst:\n",
    "    print(item)\n",
    "    print(data[item])\n",
    "    print(data[item].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7985e3b2-6e57-4a85-afd1-f4cd745ac6cb",
   "metadata": {},
   "source": [
    "## Test survival_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0968b818-975c-4a94-95bb-2c13f8be4a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = torch.linspace(1, 10, 20, dtype=torch.float32, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db7b8b17-c8c1-40cf-836a-accdfebd8dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  1.4737,  1.9474,  2.4211,  2.8947,  3.3684,  3.8421,  4.3158,\n",
       "         4.7895,  5.2632,  5.7368,  6.2105,  6.6842,  7.1579,  7.6316,  8.1053,\n",
       "         8.5789,  9.0526,  9.5263, 10.0000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60d848-19ea-4479-9cb3-423646e0fb2b",
   "metadata": {},
   "source": [
    "## Test using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a54b4ab8-1b9c-48be-b082-f92f95ec80ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d9038dd-420c-499c-885b-e4a445e7e22c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global_step': 287,\n",
       " 'metric': 0.6911187879741192,\n",
       " 'model_state_dict': OrderedDict([('model.odefunc.net.0.weight',\n",
       "               tensor([[ 5.7465e-01,  1.4183e-01, -1.2239e-01,  7.9226e-02, -4.4663e-02,\n",
       "                         5.3804e-02,  2.6958e-02, -2.4417e-02,  1.0042e-01,  7.7061e-02,\n",
       "                         7.2137e-02,  1.1669e-01,  5.3177e-03, -1.0508e-01,  2.0999e-02,\n",
       "                        -1.6629e-01,  7.5363e-02,  9.7024e-02,  1.5068e-01,  1.2049e-03,\n",
       "                        -1.1039e-01, -7.6507e-02, -2.3971e-02,  2.2556e-02,  2.0799e-02,\n",
       "                        -4.5283e-02,  5.4340e-02,  1.7199e-01, -2.2716e-01],\n",
       "                       [ 4.4473e-01,  3.9135e-01,  2.0771e-01,  7.6256e-02,  3.7924e-02,\n",
       "                         2.0954e-03, -4.3050e-02,  1.9716e-02, -3.2145e-02,  6.7171e-02,\n",
       "                        -1.5756e-01,  1.2031e-01,  2.4916e-02,  2.4038e-01,  2.2760e-02,\n",
       "                         5.8095e-02,  1.6295e-01, -7.4243e-03,  7.6385e-02,  4.4061e-02,\n",
       "                        -2.4678e-02, -1.9511e-01, -1.2932e-01,  1.4055e-01,  1.0953e-02,\n",
       "                         8.4773e-02, -4.5415e-03, -2.8751e-01, -2.5498e-01],\n",
       "                       [ 1.7118e-02,  2.7075e-03, -3.2755e-01,  8.1906e-02,  9.2869e-02,\n",
       "                         6.9415e-02, -7.5019e-02,  1.2350e-01, -3.1066e-02, -7.6552e-03,\n",
       "                        -7.7111e-02, -1.9170e-01,  1.3480e-02, -1.4890e-01, -9.4804e-02,\n",
       "                        -1.2386e-01, -8.5528e-02, -1.2057e-02, -1.4105e-02, -1.5957e-02,\n",
       "                        -1.7452e-01,  7.9409e-02,  3.9866e-02, -2.4050e-01, -2.0437e-02,\n",
       "                        -1.1477e-01,  7.8204e-02,  2.1142e-01, -4.0248e-02],\n",
       "                       [ 3.9732e-01, -5.8453e-03,  5.4857e-03,  6.7558e-02, -1.1152e-01,\n",
       "                         1.4171e-02,  8.1354e-02, -7.0695e-02, -3.7396e-02,  6.6453e-02,\n",
       "                         4.9947e-02, -1.3993e-01, -1.0668e-02,  5.3952e-03, -6.3220e-02,\n",
       "                         5.8652e-02,  6.7108e-02, -1.8939e-01, -2.2718e-01,  1.1995e-01,\n",
       "                         3.3740e-02,  1.3304e-01,  4.6077e-02,  5.0187e-02,  9.6191e-02,\n",
       "                         8.6430e-02, -1.9943e-01,  3.4182e-01,  2.8742e-01],\n",
       "                       [ 4.2684e-01,  1.4299e-01, -6.0091e-02, -6.5537e-02, -7.6112e-03,\n",
       "                         8.3940e-02,  1.8916e-01, -1.0907e-01,  7.0699e-02, -2.0408e-02,\n",
       "                        -2.6153e-02, -7.1428e-02,  1.1086e-01,  9.2197e-02, -1.6506e-01,\n",
       "                         1.1795e-01,  1.3377e-01,  8.8741e-02,  1.1760e-01,  5.3485e-02,\n",
       "                         6.3350e-02,  1.3426e-01, -1.4938e-01, -2.4048e-01, -1.3941e-01,\n",
       "                        -7.0872e-02, -3.3256e-01,  2.5204e-01,  2.5750e-01],\n",
       "                       [ 2.0790e-01,  1.0993e-01,  6.7779e-02, -2.7236e-02,  3.7475e-02,\n",
       "                        -7.5631e-02,  1.0089e-01,  3.1201e-03,  3.7463e-02, -2.8401e-01,\n",
       "                        -3.6632e-02, -1.8163e-01, -7.5886e-02, -3.6921e-03, -5.6230e-02,\n",
       "                        -1.4323e-01,  3.6176e-02, -4.9679e-02,  4.6443e-02, -2.1150e-01,\n",
       "                         3.3655e-02,  1.9815e-02, -5.8787e-02, -2.6364e-01, -2.0028e-01,\n",
       "                        -4.4039e-02,  8.9130e-02,  3.2650e-01,  2.5707e-01],\n",
       "                       [ 5.3683e-01,  2.6477e-01, -2.8354e-02, -9.3326e-02, -1.4313e-01,\n",
       "                        -2.8689e-02,  2.7863e-03,  3.3429e-02,  3.7651e-02, -1.4423e-01,\n",
       "                        -1.1085e-02,  1.5171e-01, -1.0723e-01, -1.8659e-01, -4.0960e-02,\n",
       "                        -9.6792e-02,  7.3442e-02, -3.1011e-01, -3.2577e-01, -2.2801e-01,\n",
       "                         2.7544e-01, -2.6937e-01,  6.3205e-02,  9.3904e-02,  1.1084e-01,\n",
       "                        -6.1114e-02,  2.7881e-02,  9.0715e-04,  1.0281e-01],\n",
       "                       [-8.1236e-02,  1.0717e-01, -1.6612e-01,  2.0446e-02,  1.0623e-01,\n",
       "                        -2.3654e-02,  2.0323e-01,  8.2047e-02,  5.7555e-02, -1.6011e-01,\n",
       "                         1.6708e-01,  9.8108e-02,  1.0459e-01,  2.5494e-02, -1.2958e-01,\n",
       "                        -8.7240e-02,  9.7526e-02, -1.8202e-01,  1.0010e-01, -1.8631e-01,\n",
       "                        -5.3746e-02,  2.3642e-02, -6.4953e-03, -4.3643e-02,  2.4256e-02,\n",
       "                         1.8742e-01,  1.8438e-01,  2.5525e-01, -1.8530e-01],\n",
       "                       [-2.4343e-01, -3.6176e-01, -1.9023e-01, -1.7809e-01, -6.5101e-02,\n",
       "                         9.8125e-02,  9.4349e-02, -2.4118e-01,  7.3063e-03, -1.0089e-02,\n",
       "                         7.8738e-02, -1.8580e-02,  4.9555e-02, -3.1019e-02, -2.2239e-01,\n",
       "                        -1.9680e-01, -2.6715e-01,  1.0336e-01,  1.2530e-01, -2.1268e-01,\n",
       "                        -1.1538e-01,  1.1000e-01,  1.7253e-01, -2.4275e-01,  9.4193e-02,\n",
       "                         2.5191e-01,  3.9841e-02, -1.0237e-01,  1.5493e-01],\n",
       "                       [-1.5302e-01, -3.1612e-01,  5.1612e-02, -4.9459e-02, -6.2529e-02,\n",
       "                         2.2907e-01, -3.5135e-01,  2.7596e-01,  5.0254e-02,  6.2157e-02,\n",
       "                         4.0386e-02, -3.6236e-02,  1.3517e-01, -5.2673e-02, -1.6094e-01,\n",
       "                        -8.6780e-02,  1.3866e-02, -1.3569e-01, -4.0697e-02, -8.6425e-02,\n",
       "                         1.0010e-01,  1.9214e-01,  2.0408e-02, -1.0742e-01, -1.3877e-01,\n",
       "                        -2.8439e-02,  1.2856e-01, -1.4138e-01, -3.9448e-02],\n",
       "                       [-5.1750e-01, -5.6656e-01, -6.8847e-02,  9.6561e-02,  1.4152e-03,\n",
       "                         5.4341e-02,  4.1524e-02, -4.4522e-02,  2.8916e-02,  6.8418e-02,\n",
       "                         3.3850e-02,  1.3342e-02, -1.1455e-01, -5.3654e-02,  1.1266e-02,\n",
       "                        -7.1407e-03, -2.3425e-01,  1.6384e-01,  1.2100e-02, -4.7269e-02,\n",
       "                        -6.9126e-03, -9.3309e-02, -3.9933e-01, -2.9524e-01,  1.5466e-01,\n",
       "                        -1.3640e-01, -1.9863e-01,  2.3898e-01,  2.1062e-01],\n",
       "                       [-1.5714e-01, -1.6171e-01, -1.0578e-01,  2.4981e-03, -1.2519e-02,\n",
       "                         1.6055e-01, -1.5406e-01,  4.4909e-03,  3.5074e-02, -1.8773e-01,\n",
       "                        -3.3115e-02, -1.5809e-02, -3.1480e-04,  2.0784e-02, -3.4993e-01,\n",
       "                        -1.4588e-01, -2.9770e-01, -3.4665e-01, -3.4708e-01, -1.9588e-01,\n",
       "                         1.2681e-01, -1.0903e-01, -3.8193e-01, -3.9670e-01,  9.0749e-02,\n",
       "                         8.7844e-02, -9.6523e-02,  2.9541e-01,  3.8800e-02],\n",
       "                       [-7.7036e-01, -6.1600e-01,  2.0227e-01, -3.3676e-02, -1.1521e-01,\n",
       "                        -3.6599e-02,  2.4966e-01,  3.4398e-02,  1.1108e-01,  6.9919e-02,\n",
       "                         1.0424e-01, -3.0222e-02, -1.5029e-01, -3.6669e-01,  5.0900e-02,\n",
       "                        -2.0486e-02,  6.1335e-02, -2.5576e-01, -4.4739e-01, -3.6522e-01,\n",
       "                         7.1414e-02, -5.3321e-02, -4.2429e-01,  4.1755e-02, -2.6694e-01,\n",
       "                        -1.0013e-02,  9.7795e-02,  2.7259e-01, -2.5085e-01],\n",
       "                       [ 4.4076e-01,  1.4464e-01, -3.0946e-02,  2.4709e-02, -1.0331e-01,\n",
       "                        -5.8667e-03,  4.7693e-02, -6.4140e-02,  1.4736e-02,  6.9226e-02,\n",
       "                         2.5818e-02, -4.3554e-02, -4.3101e-02, -6.5969e-02,  5.2869e-02,\n",
       "                         9.7987e-03, -1.8671e-01,  2.0584e-01, -1.0758e-01,  4.6690e-02,\n",
       "                         1.5988e-01,  1.0394e-01,  5.3030e-02, -1.8072e-01,  9.0460e-02,\n",
       "                        -3.0099e-02, -2.9122e-01,  7.5892e-02,  2.1368e-02],\n",
       "                       [-5.6152e-01, -5.3600e-01,  2.0316e-01, -1.2717e-01,  1.0709e-01,\n",
       "                         1.0436e-01, -3.2776e-02, -2.3380e-01,  1.7076e-01,  8.3204e-02,\n",
       "                        -9.8303e-02,  5.4762e-02, -1.4916e-01, -1.2145e-02, -3.5792e-01,\n",
       "                         8.6623e-02, -2.1991e-02,  6.6103e-02, -2.4760e-01, -1.5881e-01,\n",
       "                        -5.2558e-02, -1.3580e-01,  1.4492e-01, -5.4137e-02, -2.7852e-01,\n",
       "                        -1.8597e-01,  4.6672e-02,  9.9442e-02,  1.5339e-01],\n",
       "                       [-5.6086e-01, -1.9964e-01, -1.3498e-02,  6.5604e-02,  3.3979e-01,\n",
       "                        -1.0381e-01,  2.8223e-01, -1.3899e-01, -9.1084e-03,  8.2663e-02,\n",
       "                         2.0922e-02, -8.0042e-02,  1.8467e-01, -1.8490e-01, -1.3241e-01,\n",
       "                        -8.8097e-02, -1.8278e-02,  1.0233e-01, -2.6783e-01, -7.1425e-02,\n",
       "                         6.9884e-02, -1.2368e-01, -6.5008e-02,  6.0559e-02, -3.5022e-01,\n",
       "                         1.7210e-01, -2.1952e-01,  9.0464e-02,  2.4243e-01],\n",
       "                       [-3.9462e-01, -6.9538e-02, -2.2103e-01, -4.6071e-01,  5.0347e-02,\n",
       "                        -1.7471e-01, -9.0288e-02, -4.6139e-02,  2.2523e-01,  2.2325e-01,\n",
       "                         1.5165e-01,  8.3599e-02, -6.7495e-02,  4.3360e-04, -7.8266e-02,\n",
       "                        -4.2022e-01, -8.3356e-03, -3.5492e-01, -2.2434e-01, -1.9216e-01,\n",
       "                         6.8715e-02,  1.7785e-02, -3.8457e-01,  1.3771e-01,  1.7934e-02,\n",
       "                        -4.5373e-02,  9.4317e-02,  3.7920e-02,  3.2438e-01],\n",
       "                       [-3.0034e-01,  3.7899e-02,  1.8500e-01,  1.9044e-01, -7.9987e-03,\n",
       "                         1.0527e-02, -1.6415e-02, -1.7323e-01, -6.8669e-02, -3.4969e-02,\n",
       "                         7.6096e-02,  2.4735e-05, -1.5429e-01, -8.0843e-02, -1.9858e-01,\n",
       "                         6.8803e-02, -1.5476e-02, -2.9457e-01,  1.2447e-02, -3.1609e-01,\n",
       "                         2.1967e-01,  8.6994e-02, -3.5993e-01, -3.2319e-01, -2.9986e-01,\n",
       "                        -3.0556e-02,  5.3473e-02, -2.8123e-01, -3.1850e-01],\n",
       "                       [ 2.4375e-01, -9.8732e-04, -2.5260e-01, -8.4422e-02,  1.4915e-01,\n",
       "                        -2.5186e-02, -6.3578e-03, -1.2611e-02,  7.5309e-02, -4.6904e-03,\n",
       "                         7.3267e-02,  5.3871e-03, -1.6085e-01, -3.2672e-01,  7.9051e-02,\n",
       "                        -1.0837e-01, -2.0852e-01,  1.8317e-01,  9.6109e-03, -2.3084e-01,\n",
       "                        -3.7784e-02, -9.0019e-02, -3.6758e-02,  4.8570e-02, -2.1757e-01,\n",
       "                         7.3337e-02, -1.1277e-01,  4.7108e-01,  1.0061e-01],\n",
       "                       [ 1.3229e-01,  1.6280e-01, -1.2702e-01, -5.3304e-02, -3.1147e-02,\n",
       "                        -6.9880e-04, -1.7551e-02, -7.7867e-02, -8.7338e-02, -1.4609e-01,\n",
       "                        -1.5693e-02, -5.0740e-02, -9.2703e-02,  6.8438e-03, -1.1011e-01,\n",
       "                        -2.2391e-01, -1.7929e-01,  6.8413e-02, -7.6074e-02,  6.8266e-02,\n",
       "                        -3.9564e-02, -8.6263e-02, -9.1689e-02, -7.9472e-02,  2.9088e-02,\n",
       "                        -1.3234e-02, -9.1961e-02,  3.0852e-01,  1.3028e-01],\n",
       "                       [ 3.6626e-01,  6.8639e-02, -1.1771e-01,  1.2672e-01,  3.5827e-02,\n",
       "                        -1.2601e-01, -2.3174e-02, -1.4710e-02,  2.1021e-02,  1.1945e-01,\n",
       "                         4.3790e-02,  1.5366e-01,  8.5267e-03,  3.6692e-02, -8.8898e-02,\n",
       "                         8.6881e-02,  9.6451e-03, -1.3384e-01, -8.8544e-02,  1.0666e-01,\n",
       "                         1.3029e-01,  3.0293e-02,  1.0308e-01,  8.8363e-02, -1.8479e-01,\n",
       "                        -1.9364e-01,  5.7607e-02,  3.3562e-01,  2.4765e-02],\n",
       "                       [-3.2250e-01, -2.7819e-01, -8.8472e-02, -2.0476e-01,  1.5308e-01,\n",
       "                         1.9570e-01,  1.5954e-01,  1.6879e-01,  1.2500e-02,  1.1925e-01,\n",
       "                        -2.0633e-02,  1.1448e-02,  4.4941e-02,  1.2589e-01,  5.2379e-02,\n",
       "                         1.2026e-01,  9.1703e-02, -2.3164e-01,  1.0533e-01, -2.2765e-01,\n",
       "                         1.5652e-01,  1.3466e-01, -3.1484e-02, -3.8525e-03, -2.7121e-01,\n",
       "                        -2.5450e-01, -2.6515e-02, -7.9232e-02, -2.6419e-02],\n",
       "                       [-7.2518e-01, -5.5787e-01,  6.5239e-02,  8.3323e-02,  2.3366e-01,\n",
       "                         2.1303e-01,  3.8029e-02,  1.1399e-01, -1.6509e-01,  1.9953e-01,\n",
       "                         2.4445e-02, -9.9578e-02, -9.3996e-02, -6.0421e-02, -2.5693e-01,\n",
       "                        -2.8511e-02, -2.9766e-01, -3.2187e-01, -2.9433e-01, -2.6305e-02,\n",
       "                         4.5035e-03,  3.4554e-03, -2.6460e-01,  1.3373e-01, -2.4819e-01,\n",
       "                        -1.8369e-01,  1.1466e-01, -5.8628e-03,  1.8513e-01],\n",
       "                       [-4.7456e-01, -4.8322e-01, -5.4269e-02, -1.5348e-01,  2.4584e-02,\n",
       "                        -1.7605e-01, -1.6591e-03, -8.0428e-02,  1.7420e-01, -4.1516e-02,\n",
       "                        -9.3174e-03,  1.2754e-01, -8.5936e-02,  3.0674e-02,  1.1182e-01,\n",
       "                        -4.5855e-01, -4.1428e-01, -3.0626e-01, -5.0353e-01, -2.7135e-01,\n",
       "                        -1.0028e-01, -1.3938e-02, -4.4334e-01,  3.0926e-02, -4.4697e-01,\n",
       "                        -7.1751e-02, -2.9842e-01, -1.3248e-01, -6.5865e-02],\n",
       "                       [-2.9243e-01, -3.6920e-01,  1.1958e-01,  6.6563e-02, -1.0979e-01,\n",
       "                        -1.4719e-01, -2.2232e-01,  1.1671e-01,  1.8256e-01, -8.3808e-02,\n",
       "                         2.0781e-01,  1.2388e-01,  6.2377e-02,  2.1620e-02, -2.5912e-01,\n",
       "                        -4.6398e-03,  8.9554e-02, -2.1792e-01, -2.3010e-01, -1.2207e-01,\n",
       "                        -8.6158e-02,  1.1414e-01,  1.2472e-02, -2.3201e-01,  2.1481e-01,\n",
       "                         2.4099e-01,  5.7235e-02, -8.2137e-02,  2.5158e-02],\n",
       "                       [ 3.6883e-01,  7.3578e-02, -2.1086e-01,  2.7584e-02,  3.8753e-02,\n",
       "                         6.5779e-02,  1.3010e-01,  1.2233e-02, -3.3602e-02,  1.4427e-01,\n",
       "                         1.5549e-01,  5.3064e-02, -1.8640e-01, -7.5908e-02, -1.6199e-01,\n",
       "                        -1.9356e-01, -2.1292e-01,  1.0054e-01, -1.1661e-02, -2.7198e-01,\n",
       "                         1.1277e-01,  1.6982e-01,  1.1614e-01,  6.8291e-02, -1.2726e-01,\n",
       "                         7.9400e-02,  1.3985e-01,  2.8738e-01, -3.5619e-02]], device='cuda:0')),\n",
       "              ('model.odefunc.net.0.bias',\n",
       "               tensor([-0.0014,  0.0417,  0.2564,  0.1879,  0.0072,  0.0789,  0.1003, -0.0651,\n",
       "                        0.2923, -0.1049,  0.1789,  0.2823,  0.1626,  0.1549,  0.2299,  0.3228,\n",
       "                        0.1119,  0.2031, -0.0842,  0.1887,  0.2663,  0.1239,  0.2352,  0.2330,\n",
       "                        0.0016,  0.0152], device='cuda:0')),\n",
       "              ('model.odefunc.net.2.weight',\n",
       "               tensor([[-0.2117,  0.0530,  0.0122, -0.1560, -0.3807, -0.1654, -0.3687, -0.1030,\n",
       "                         0.2761,  0.2661,  0.1582,  0.1032,  0.4263, -0.1402,  0.5390,  0.2647,\n",
       "                         0.2478,  0.0777, -0.0823, -0.1743, -0.0955,  0.3869,  0.4174,  0.3741,\n",
       "                         0.1641, -0.2617],\n",
       "                       [ 0.0716, -0.0121, -0.0398,  0.0526,  0.0971,  0.0336,  0.1165,  0.0160,\n",
       "                         0.0138,  0.1081,  0.0069,  0.3248, -0.1560,  0.0548, -0.0013,  0.1046,\n",
       "                        -0.0054, -0.0658,  0.2682,  0.0076,  0.0460, -0.1014, -0.1822, -0.1856,\n",
       "                         0.0634,  0.2494],\n",
       "                       [ 0.0594,  0.1027,  0.1975, -0.1561,  0.2343, -0.1045,  0.0175,  0.2611,\n",
       "                         0.2606,  0.0825,  0.1682,  0.0141,  0.5074, -0.0772,  0.2440,  0.3659,\n",
       "                         0.1531,  0.4214, -0.4299,  0.0812, -0.0809,  0.1975,  0.2215,  0.3193,\n",
       "                        -0.0181, -0.1064],\n",
       "                       [-0.0236, -0.2511,  0.2312,  0.1579, -0.0046,  0.2922,  0.0267,  0.1401,\n",
       "                        -0.0222, -0.0702, -0.0924,  0.3002, -0.3105,  0.1216, -0.0174,  0.0187,\n",
       "                         0.1571, -0.1365,  0.1268,  0.1437,  0.0138, -0.1537, -0.1599, -0.2823,\n",
       "                         0.1118,  0.2813],\n",
       "                       [ 0.1940, -0.2339,  0.1556,  0.2915,  0.0570,  0.2050,  0.1732,  0.1547,\n",
       "                         0.1094,  0.1008, -0.2534,  0.0785, -0.3318,  0.1846, -0.1700, -0.0344,\n",
       "                        -0.0320, -0.1282,  0.1537, -0.0544,  0.2409, -0.1198, -0.1117, -0.1159,\n",
       "                        -0.1257,  0.2291],\n",
       "                       [-0.0115, -0.0418,  0.1703,  0.3168,  0.0934,  0.2633,  0.2196,  0.0632,\n",
       "                        -0.0411, -0.0162, -0.1569,  0.2175, -0.4097,  0.1656, -0.1533, -0.1854,\n",
       "                         0.0986, -0.0250,  0.2593,  0.2178,  0.0906, -0.1473, -0.1286,  0.0035,\n",
       "                        -0.1093,  0.1939],\n",
       "                       [ 0.0829, -0.1777,  0.0554,  0.2181, -0.0878,  0.2241,  0.1559,  0.0984,\n",
       "                         0.1622, -0.0660, -0.3412,  0.1344, -0.3072,  0.0960, -0.0626, -0.1526,\n",
       "                         0.0359,  0.0124,  0.0603,  0.0309,  0.2660,  0.0770, -0.1232, -0.1856,\n",
       "                        -0.0766, -0.0677],\n",
       "                       [-0.4052, -0.3433,  0.1312, -0.0911, -0.1477, -0.2080, -0.0883, -0.0519,\n",
       "                         0.2722,  0.1581,  0.3019, -0.0071,  0.4510, -0.4230,  0.4658,  0.1805,\n",
       "                         0.2216,  0.2432, -0.1248,  0.0094, -0.0890,  0.2995,  0.2457,  0.2522,\n",
       "                         0.1570, -0.0623],\n",
       "                       [-0.3019, -0.1953,  0.1085,  0.0202, -0.0453, -0.2400, -0.3944,  0.1845,\n",
       "                         0.1961,  0.0430,  0.2455,  0.2607,  0.4351, -0.3377,  0.3021,  0.1856,\n",
       "                         0.1257,  0.2433,  0.0778, -0.1144, -0.1262,  0.0389,  0.5562,  0.4199,\n",
       "                         0.3158, -0.1019],\n",
       "                       [-0.2425, -0.0009,  0.0792, -0.2698, -0.2104, -0.2080, -0.5200, -0.0507,\n",
       "                         0.1894,  0.1608,  0.4910,  0.1587,  0.4932, -0.2785,  0.3195,  0.3107,\n",
       "                         0.1691,  0.0738,  0.1172,  0.0425, -0.0186,  0.2391,  0.2924,  0.3187,\n",
       "                         0.2320, -0.2248],\n",
       "                       [-0.0995, -0.0132, -0.0348,  0.3259,  0.1941,  0.2601,  0.1674,  0.0925,\n",
       "                         0.0027,  0.0733, -0.2465,  0.0340, -0.1244,  0.0407, -0.1039, -0.0567,\n",
       "                         0.1116, -0.2900,  0.0589,  0.2368,  0.1701,  0.0537, -0.2733, -0.1604,\n",
       "                        -0.1190,  0.0918],\n",
       "                       [-0.2310, -0.2484, -0.1346, -0.1771, -0.0585,  0.0314, -0.3333, -0.0118,\n",
       "                         0.0465,  0.1063,  0.2605,  0.2925,  0.3739, -0.2025,  0.5684,  0.2689,\n",
       "                         0.2765,  0.2499,  0.0961, -0.0341, -0.2993,  0.0639,  0.2796,  0.3840,\n",
       "                         0.2825, -0.1134],\n",
       "                       [-0.3149, -0.1732,  0.2763, -0.1493, -0.3934, -0.0882, -0.2906,  0.1173,\n",
       "                         0.2923,  0.2918,  0.1951,  0.1661,  0.3679, -0.1006,  0.5016,  0.4255,\n",
       "                         0.3927,  0.2363, -0.2538,  0.0053, -0.2004,  0.2847,  0.3705,  0.4478,\n",
       "                         0.1626, -0.3433],\n",
       "                       [-0.0193, -0.1155,  0.1957,  0.3223,  0.2082,  0.1834, -0.0044, -0.0484,\n",
       "                        -0.0562,  0.0868, -0.2997,  0.2385, -0.1522,  0.1979, -0.2238,  0.0982,\n",
       "                        -0.0304, -0.1871,  0.3710,  0.1888,  0.0426,  0.1091, -0.2754, -0.1051,\n",
       "                         0.1425,  0.1018],\n",
       "                       [-0.0022, -0.0925, -0.0977, -0.0718, -0.0431, -0.1791, -0.1446,  0.0610,\n",
       "                        -0.0372, -0.0943, -0.0468,  0.0051,  0.1606, -0.1327, -0.1586,  0.0572,\n",
       "                        -0.2519,  0.0012,  0.1467, -0.1840, -0.2533,  0.1655, -0.0902,  0.0736,\n",
       "                        -0.0211, -0.1839],\n",
       "                       [ 0.0434, -0.2537,  0.1521,  0.0663,  0.0349, -0.0048,  0.1112,  0.1634,\n",
       "                        -0.0166, -0.1278, -0.3753,  0.3232, -0.1741,  0.1034, -0.1364,  0.0354,\n",
       "                         0.1146, -0.0559,  0.2888,  0.0830,  0.2444,  0.0533, -0.1645, -0.2408,\n",
       "                        -0.0311,  0.2581],\n",
       "                       [ 0.1771, -0.2231,  0.0383,  0.2377,  0.1437,  0.3049,  0.1265,  0.0811,\n",
       "                         0.0437, -0.1395, -0.2773,  0.1841, -0.3627, -0.1296, -0.0542, -0.0378,\n",
       "                        -0.0761, -0.2412,  0.3395,  0.1888,  0.2684, -0.1289, -0.1057, -0.3301,\n",
       "                         0.0185, -0.0016],\n",
       "                       [-0.0436,  0.0385,  0.1051, -0.1105, -0.0834, -0.0471, -0.0358, -0.1323,\n",
       "                         0.1247,  0.0774,  0.1124, -0.1609,  0.1218, -0.0038, -0.0627, -0.1236,\n",
       "                        -0.0065, -0.1354,  0.0948, -0.0103,  0.0880, -0.1780, -0.0978, -0.1717,\n",
       "                         0.0630,  0.2098],\n",
       "                       [-0.1363, -0.2790,  0.2124,  0.2092,  0.1828, -0.0285,  0.1097,  0.1058,\n",
       "                        -0.0525, -0.0502, -0.0261,  0.1853, -0.1813,  0.0246, -0.2304, -0.1623,\n",
       "                         0.1999, -0.1946,  0.3526,  0.2231, -0.0252, -0.0509, -0.2264, -0.2064,\n",
       "                         0.0669,  0.2075],\n",
       "                       [-0.0609, -0.3190, -0.0027,  0.0079,  0.1634,  0.0442,  0.0593, -0.0183,\n",
       "                         0.0767,  0.0361, -0.0253,  0.1870, -0.1132,  0.1022, -0.1313,  0.0101,\n",
       "                         0.1512, -0.0890,  0.3388,  0.1175,  0.1925,  0.0827, -0.2852, -0.2562,\n",
       "                         0.1598,  0.0928],\n",
       "                       [-0.0448, -0.2157,  0.0896,  0.2094,  0.1047,  0.2196,  0.0134,  0.0729,\n",
       "                        -0.0253,  0.1381, -0.2559,  0.0662, -0.2299,  0.1343, -0.1197, -0.0310,\n",
       "                         0.0595, -0.0122,  0.1975, -0.0761, -0.0690,  0.0078, -0.1214, -0.0349,\n",
       "                        -0.0413,  0.2689],\n",
       "                       [-0.1684, -0.1615, -0.0519, -0.0820, -0.0457,  0.0355, -0.1740,  0.1109,\n",
       "                        -0.1010,  0.1147,  0.0528, -0.0496, -0.2130,  0.0042, -0.1398,  0.1892,\n",
       "                        -0.1098, -0.1331, -0.1239, -0.1646, -0.1005, -0.0182, -0.0253,  0.0192,\n",
       "                         0.0956, -0.0715],\n",
       "                       [-0.2290, -0.0875,  0.2801, -0.1720, -0.4039,  0.0227, -0.2188,  0.0918,\n",
       "                         0.2082,  0.3383,  0.2367,  0.2635,  0.6178, -0.3292,  0.5481,  0.3419,\n",
       "                         0.3314,  0.0940, -0.2456, -0.1559, -0.1868,  0.2471,  0.5423,  0.4445,\n",
       "                         0.2858, -0.1709],\n",
       "                       [ 0.0460,  0.0709,  0.1683,  0.0586,  0.0106,  0.1420,  0.0507, -0.1426,\n",
       "                        -0.0708,  0.0834,  0.0762,  0.0288, -0.1861, -0.0360, -0.1145,  0.1625,\n",
       "                         0.0107, -0.0156, -0.0296, -0.0100,  0.0436,  0.0088, -0.1331, -0.1866,\n",
       "                        -0.0546, -0.0518],\n",
       "                       [-0.0840, -0.2378,  0.2670,  0.0307, -0.0971,  0.2164,  0.1341,  0.1284,\n",
       "                        -0.1085, -0.1120, -0.4268,  0.1239, -0.2634,  0.1417, -0.0524, -0.1468,\n",
       "                         0.1825, -0.1521,  0.2503,  0.1516,  0.1549,  0.0156, -0.1375, -0.0669,\n",
       "                        -0.0163,  0.2958],\n",
       "                       [ 0.0034, -0.0603,  0.1294, -0.1912,  0.0790, -0.1904,  0.1236, -0.0424,\n",
       "                         0.2248,  0.0337,  0.3961,  0.0200,  0.5088, -0.2501,  0.1501,  0.3051,\n",
       "                         0.0799,  0.3367, -0.1915, -0.1356, -0.0835,  0.2921,  0.4013,  0.1772,\n",
       "                         0.0901, -0.1055]], device='cuda:0')),\n",
       "              ('model.odefunc.net.2.bias',\n",
       "               tensor([ 0.2489, -0.1216,  0.1096, -0.0372,  0.0821,  0.0177,  0.1077,  0.1893,\n",
       "                       -0.0265,  0.1325, -0.0031,  0.1167,  0.1565, -0.0819, -0.0947,  0.0055,\n",
       "                        0.1764, -0.0951,  0.1343, -0.0526,  0.0350, -0.1947, -0.0343,  0.0299,\n",
       "                        0.0018,  0.2549], device='cuda:0')),\n",
       "              ('model.odefunc.net.4.weight',\n",
       "               tensor([[ 0.5554, -0.1872,  0.3423, -0.2360, -0.2366, -0.2420, -0.1795,  0.3993,\n",
       "                         0.5240,  0.6892, -0.0981,  0.4895,  0.4863, -0.1729,  0.0374, -0.3192,\n",
       "                        -0.1444, -0.0466, -0.2052, -0.2105, -0.1525, -0.1513,  0.4275,  0.0053,\n",
       "                        -0.2128,  0.4719]], device='cuda:0')),\n",
       "              ('model.odefunc.net.4.bias',\n",
       "               tensor([0.1910], device='cuda:0'))]),\n",
       " 'optimizer_state_dict': {'state': {0: {'step': 287,\n",
       "    'square_avg': tensor([[2.3306e-05, 1.5580e-05, 1.0378e-04, 8.5938e-05, 8.2250e-05, 8.1764e-05,\n",
       "             1.1094e-04, 9.7385e-05, 7.9749e-05, 7.0759e-05, 8.5992e-05, 9.0137e-05,\n",
       "             1.0390e-04, 6.1469e-05, 9.1337e-05, 1.0814e-05, 6.1814e-05, 1.1256e-04,\n",
       "             4.8966e-05, 1.8188e-08, 1.0882e-04, 1.0096e-04, 1.7874e-04, 7.2326e-05,\n",
       "             1.9685e-04, 8.6556e-05, 1.0021e-04, 7.6830e-05, 2.4723e-05],\n",
       "            [9.6764e-05, 3.8303e-04, 8.5424e-05, 8.2525e-05, 8.3492e-05, 8.7861e-05,\n",
       "             9.0742e-05, 7.8180e-05, 7.4351e-05, 8.2645e-05, 1.1626e-04, 1.0344e-04,\n",
       "             8.9742e-05, 1.2114e-04, 1.1591e-04, 1.1949e-04, 1.0701e-04, 5.7238e-05,\n",
       "             9.2606e-05, 1.8895e-08, 1.0439e-04, 9.8412e-05, 7.6565e-05, 5.1314e-05,\n",
       "             7.5356e-05, 1.1979e-04, 6.0101e-05, 1.0299e-04, 1.1673e-04],\n",
       "            [2.8142e-05, 1.3801e-04, 2.7576e-05, 2.4027e-05, 2.8762e-05, 2.9287e-05,\n",
       "             3.2088e-05, 2.9609e-05, 1.8380e-05, 2.6938e-05, 3.1967e-05, 2.6303e-05,\n",
       "             4.0780e-05, 3.2905e-05, 6.0502e-05, 2.2283e-05, 1.4866e-05, 1.7127e-05,\n",
       "             1.9165e-05, 6.5120e-09, 4.1281e-05, 4.4337e-05, 3.8237e-05, 3.1500e-07,\n",
       "             3.6028e-05, 3.2799e-05, 2.5484e-05, 2.4119e-05, 1.4004e-05],\n",
       "            [1.4189e-04, 4.5465e-04, 2.0030e-04, 2.0258e-04, 1.9126e-04, 1.9305e-04,\n",
       "             1.7610e-04, 2.1186e-04, 1.4897e-04, 1.2981e-04, 2.1214e-04, 1.7717e-04,\n",
       "             1.9182e-04, 2.3193e-04, 2.2493e-04, 2.5335e-04, 3.5447e-04, 4.8181e-07,\n",
       "             8.0152e-08, 4.0330e-08, 2.3326e-04, 2.1875e-04, 3.4193e-04, 1.2595e-04,\n",
       "             3.5883e-04, 2.8765e-04, 5.1664e-05, 1.8487e-04, 2.1531e-04],\n",
       "            [1.8510e-05, 3.4451e-05, 6.2840e-05, 6.7190e-05, 6.4849e-05, 7.1777e-05,\n",
       "             6.8462e-05, 7.0054e-05, 6.9010e-05, 5.2567e-05, 6.7710e-05, 4.7885e-05,\n",
       "             8.2412e-05, 9.7970e-05, 2.6626e-05, 8.4944e-05, 5.5845e-05, 1.1023e-04,\n",
       "             4.6358e-05, 1.0659e-08, 6.6934e-05, 7.9772e-05, 1.2906e-05, 5.1552e-07,\n",
       "             8.6109e-07, 9.2106e-05, 6.8996e-06, 6.3963e-05, 7.8710e-05],\n",
       "            [1.0052e-04, 3.2941e-04, 1.9121e-04, 1.3897e-04, 1.7431e-04, 1.3047e-04,\n",
       "             1.2311e-04, 1.4317e-04, 1.0658e-04, 4.5257e-05, 1.6892e-04, 1.0957e-04,\n",
       "             1.4152e-04, 1.8163e-04, 1.4918e-04, 5.2683e-05, 2.2990e-04, 2.8568e-05,\n",
       "             3.0315e-04, 2.6859e-08, 1.5453e-04, 1.5747e-04, 1.8786e-04, 1.3006e-06,\n",
       "             6.4739e-07, 1.6365e-04, 1.2602e-04, 1.4439e-04, 1.5509e-04],\n",
       "            [8.8005e-05, 2.0591e-04, 2.8474e-04, 2.0602e-04, 1.8944e-04, 1.9392e-04,\n",
       "             2.0736e-04, 1.8812e-04, 1.6036e-04, 1.1733e-04, 2.0559e-04, 2.2703e-04,\n",
       "             2.1708e-04, 2.1016e-04, 2.5022e-04, 1.8530e-04, 1.8641e-04, 5.4348e-07,\n",
       "             9.0409e-08, 4.5196e-08, 1.3459e-04, 7.2608e-05, 4.5087e-04, 1.8929e-04,\n",
       "             4.6010e-04, 2.3071e-04, 1.5148e-04, 2.3716e-04, 2.3322e-04],\n",
       "            [5.9473e-06, 2.9332e-05, 4.9285e-06, 6.0877e-06, 5.4511e-06, 4.6399e-06,\n",
       "             5.4834e-06, 5.8554e-06, 4.2144e-06, 3.1571e-06, 5.1810e-06, 4.8089e-06,\n",
       "             5.6906e-06, 5.5211e-06, 6.0627e-06, 2.6751e-06, 1.2469e-05, 1.2834e-08,\n",
       "             1.8233e-05, 1.0673e-09, 8.0089e-06, 7.8917e-06, 8.9800e-06, 4.8698e-06,\n",
       "             6.4559e-06, 7.1787e-06, 5.1212e-06, 4.6506e-06, 1.6644e-06],\n",
       "            [3.0926e-05, 1.6961e-05, 1.1419e-04, 9.2032e-05, 9.6882e-05, 9.9712e-05,\n",
       "             9.6334e-05, 9.2061e-05, 7.8986e-05, 6.4577e-05, 9.6626e-05, 8.8881e-05,\n",
       "             1.3799e-04, 1.1933e-04, 5.2138e-05, 4.1638e-05, 1.2297e-06, 1.4345e-04,\n",
       "             1.1907e-04, 1.9932e-08, 1.3451e-04, 1.2753e-04, 2.8808e-04, 9.6517e-07,\n",
       "             2.5488e-04, 1.4255e-04, 4.3676e-05, 1.0976e-04, 1.0878e-04],\n",
       "            [1.2715e-05, 6.9029e-06, 3.8370e-05, 3.4045e-05, 3.5548e-05, 3.1821e-05,\n",
       "             4.2097e-05, 3.6752e-05, 2.9687e-05, 3.6408e-05, 3.2642e-05, 3.3409e-05,\n",
       "             5.4747e-05, 3.7385e-05, 1.1294e-05, 1.2325e-05, 3.5037e-05, 1.0850e-07,\n",
       "             1.8050e-08, 9.0233e-09, 4.1775e-05, 4.8221e-05, 3.6772e-05, 4.3693e-07,\n",
       "             2.1748e-07, 3.7540e-05, 6.6810e-05, 4.1072e-05, 3.2003e-05],\n",
       "            [3.2474e-05, 2.1477e-05, 3.2159e-04, 3.2606e-04, 2.6425e-04, 2.3708e-04,\n",
       "             2.3315e-04, 2.1999e-04, 1.7854e-04, 1.9560e-04, 2.4214e-04, 2.4684e-04,\n",
       "             2.1461e-04, 2.2653e-04, 3.0924e-04, 3.2201e-04, 1.4885e-05, 4.7723e-04,\n",
       "             6.1268e-04, 3.7502e-08, 2.5177e-04, 2.5570e-04, 6.4266e-06, 1.8059e-06,\n",
       "             8.7331e-04, 1.7491e-04, 1.5416e-05, 1.7764e-04, 2.1992e-04],\n",
       "            [5.6142e-05, 2.7746e-04, 6.9648e-05, 7.4078e-05, 6.1458e-05, 6.3949e-05,\n",
       "             7.2412e-05, 7.0000e-05, 7.1362e-05, 3.5874e-05, 9.1520e-05, 9.0132e-05,\n",
       "             8.5217e-05, 1.1185e-04, 1.0402e-05, 2.5348e-05, 9.7297e-07, 1.8965e-07,\n",
       "             3.1549e-08, 1.5772e-08, 5.8011e-05, 5.6728e-05, 2.7252e-06, 7.6374e-07,\n",
       "             9.1183e-05, 7.4681e-05, 2.3936e-05, 7.5831e-05, 5.0948e-05],\n",
       "            [1.0208e-04, 5.5595e-05, 4.4939e-04, 5.3006e-04, 4.6018e-04, 4.7609e-04,\n",
       "             5.0208e-04, 5.5220e-04, 4.4418e-04, 3.0404e-04, 4.5588e-04, 3.7557e-04,\n",
       "             4.9707e-04, 1.6833e-04, 7.7096e-04, 4.1560e-04, 6.8996e-04, 1.3229e-06,\n",
       "             2.2007e-07, 1.1001e-07, 3.4753e-04, 3.7989e-04, 1.9230e-05, 3.3009e-04,\n",
       "             2.6516e-06, 5.7231e-04, 6.5079e-04, 2.8200e-04, 1.2850e-04],\n",
       "            [1.1324e-04, 2.6568e-04, 2.4997e-04, 2.4237e-04, 2.4394e-04, 2.2322e-04,\n",
       "             2.1362e-04, 2.5168e-04, 1.8636e-04, 1.4406e-04, 2.2570e-04, 2.0177e-04,\n",
       "             2.5471e-04, 2.6392e-04, 2.4144e-04, 2.8276e-04, 1.6050e-05, 2.4199e-04,\n",
       "             1.0233e-07, 5.1628e-08, 2.6671e-04, 2.5515e-04, 4.9199e-04, 2.4869e-06,\n",
       "             4.6106e-04, 3.0432e-04, 1.5238e-05, 2.4999e-04, 2.2371e-04],\n",
       "            [8.0605e-05, 2.9068e-05, 3.2637e-04, 2.3562e-04, 2.7179e-04, 2.6436e-04,\n",
       "             3.2966e-04, 3.2153e-04, 4.0057e-04, 1.9411e-04, 2.7674e-04, 3.5682e-04,\n",
       "             3.2541e-04, 4.4534e-04, 4.2275e-05, 5.5334e-04, 2.1460e-04, 5.3398e-04,\n",
       "             1.3563e-07, 6.7802e-08, 2.9571e-04, 1.5395e-04, 9.8451e-04, 7.8879e-05,\n",
       "             1.6343e-06, 2.2709e-04, 3.0120e-04, 2.7993e-04, 2.9803e-04],\n",
       "            [2.9836e-05, 2.7224e-05, 1.8882e-04, 1.7766e-04, 1.2188e-04, 1.3872e-04,\n",
       "             1.4853e-04, 1.4509e-04, 1.2629e-04, 9.5617e-05, 1.5043e-04, 1.3583e-04,\n",
       "             2.2669e-04, 1.0694e-04, 1.1166e-04, 1.3104e-04, 1.2549e-04, 2.1787e-04,\n",
       "             5.5012e-08, 2.7529e-08, 1.0508e-04, 1.0840e-04, 1.3850e-04, 1.5141e-04,\n",
       "             6.6287e-07, 2.0166e-04, 1.1712e-05, 1.3547e-04, 1.8024e-04],\n",
       "            [2.0244e-05, 2.6476e-05, 5.0481e-05, 5.3704e-05, 3.8623e-05, 3.6623e-05,\n",
       "             5.0932e-05, 4.4697e-05, 5.9692e-05, 5.2647e-05, 5.2198e-05, 5.6534e-05,\n",
       "             4.4076e-05, 7.1742e-05, 3.9183e-05, 1.6260e-06, 4.2747e-05, 1.2057e-07,\n",
       "             2.0058e-08, 1.0027e-08, 3.9902e-05, 4.4669e-05, 2.4765e-06, 7.4913e-05,\n",
       "             6.3689e-05, 5.8743e-05, 4.8787e-05, 5.8742e-05, 6.7007e-05],\n",
       "            [8.0127e-05, 2.0423e-04, 1.4171e-04, 1.8027e-04, 1.5693e-04, 1.3264e-04,\n",
       "             1.5301e-04, 1.5213e-04, 8.0207e-05, 7.4013e-05, 1.9844e-04, 1.7884e-04,\n",
       "             1.6413e-04, 2.3382e-04, 4.0294e-05, 2.9970e-04, 1.7081e-04, 4.2506e-07,\n",
       "             3.5178e-04, 3.5349e-08, 1.3558e-04, 1.6161e-04, 6.0916e-06, 1.7117e-06,\n",
       "             8.5202e-07, 1.8194e-04, 1.6049e-04, 1.6013e-04, 9.8336e-05],\n",
       "            [1.4300e-04, 8.1137e-04, 2.3258e-04, 1.8938e-04, 2.0065e-04, 1.9003e-04,\n",
       "             1.9057e-04, 2.0701e-04, 1.7363e-04, 1.5543e-04, 2.4362e-04, 1.9984e-04,\n",
       "             2.4702e-04, 9.3883e-05, 2.9680e-04, 1.8893e-04, 2.4085e-06, 1.9151e-04,\n",
       "             5.7825e-04, 3.9042e-08, 2.6053e-04, 2.3867e-04, 4.5647e-04, 1.6392e-04,\n",
       "             9.4103e-07, 2.5764e-04, 7.4863e-05, 1.4767e-04, 8.9792e-05],\n",
       "            [4.2403e-05, 1.5252e-04, 6.5002e-05, 5.2427e-05, 6.1432e-05, 6.0448e-05,\n",
       "             5.1365e-05, 5.5445e-05, 3.0191e-05, 3.5594e-05, 6.0906e-05, 5.4778e-05,\n",
       "             6.3815e-05, 7.7259e-05, 6.2024e-05, 9.4605e-06, 1.7236e-06, 5.2506e-05,\n",
       "             2.2944e-08, 1.1632e-08, 5.8888e-05, 6.1614e-05, 9.2822e-05, 3.3683e-05,\n",
       "             1.0818e-04, 6.4275e-05, 3.6612e-05, 5.4364e-05, 5.3466e-05],\n",
       "            [9.4194e-05, 2.8587e-04, 1.4080e-04, 1.3452e-04, 1.4408e-04, 1.3317e-04,\n",
       "             1.3876e-04, 1.5441e-04, 1.1731e-04, 8.8895e-05, 1.4723e-04, 1.2366e-04,\n",
       "             1.4844e-04, 1.6444e-04, 1.0521e-04, 1.7396e-04, 1.7017e-04, 3.6308e-07,\n",
       "             6.0400e-08, 3.0440e-08, 1.5341e-04, 1.5603e-04, 2.7311e-04, 9.2900e-05,\n",
       "             7.2778e-07, 1.3034e-04, 1.1986e-04, 1.3653e-04, 1.0714e-04],\n",
       "            [2.2474e-05, 9.5644e-06, 8.4537e-05, 7.4231e-05, 9.4302e-05, 9.3282e-05,\n",
       "             7.2164e-05, 9.2423e-05, 8.3514e-05, 8.3382e-05, 7.8944e-05, 7.0982e-05,\n",
       "             8.2360e-05, 1.0859e-04, 7.6772e-05, 1.3884e-04, 1.4038e-04, 1.8512e-07,\n",
       "             1.8935e-04, 1.5395e-08, 7.2359e-05, 8.1663e-05, 3.6686e-05, 1.3463e-05,\n",
       "             3.7107e-07, 5.4350e-05, 4.4025e-05, 9.7592e-05, 8.2710e-05],\n",
       "            [1.0666e-04, 4.0145e-05, 3.7928e-04, 3.9379e-04, 3.3298e-04, 3.5881e-04,\n",
       "             3.6132e-04, 3.2078e-04, 1.7720e-04, 2.7993e-04, 2.5643e-04, 3.0225e-04,\n",
       "             3.6635e-04, 4.5416e-04, 1.0245e-04, 2.8725e-04, 4.2357e-06, 8.0194e-07,\n",
       "             1.3341e-07, 6.7205e-08, 3.5267e-04, 3.8095e-04, 1.6978e-05, 4.9272e-04,\n",
       "             1.6075e-06, 2.2059e-04, 3.7854e-04, 3.6866e-04, 3.5670e-04],\n",
       "            [9.3361e-05, 4.7487e-05, 4.1473e-04, 4.0793e-04, 4.4848e-04, 3.6756e-04,\n",
       "             4.5630e-04, 4.4285e-04, 4.4522e-04, 2.7576e-04, 5.1454e-04, 4.2503e-04,\n",
       "             4.3762e-04, 5.5534e-04, 5.9536e-04, 1.4588e-05, 5.3885e-06, 1.0503e-06,\n",
       "             1.7473e-07, 8.7349e-08, 4.7897e-04, 5.1940e-04, 2.0622e-05, 3.6470e-04,\n",
       "             2.1054e-06, 4.2657e-04, 1.8447e-05, 4.3170e-04, 3.4397e-04],\n",
       "            [1.1974e-05, 4.6468e-06, 4.2546e-05, 4.1455e-05, 3.3033e-05, 4.3625e-05,\n",
       "             5.4893e-05, 5.1722e-05, 5.4387e-05, 2.5719e-05, 5.0068e-05, 4.4654e-05,\n",
       "             5.9953e-05, 5.6848e-05, 1.0626e-05, 6.0241e-05, 6.2543e-05, 1.0630e-07,\n",
       "             1.7683e-08, 8.8401e-09, 6.1780e-05, 6.9921e-05, 5.9241e-05, 4.2808e-07,\n",
       "             1.6080e-04, 8.1130e-05, 5.6330e-05, 5.4437e-05, 4.1047e-05],\n",
       "            [1.3621e-04, 5.0141e-04, 2.2535e-04, 1.9582e-04, 2.3376e-04, 2.2386e-04,\n",
       "             2.0355e-04, 2.4032e-04, 1.4570e-04, 1.6476e-04, 2.6145e-04, 1.8484e-04,\n",
       "             2.2308e-04, 2.8455e-04, 1.9614e-04, 6.1963e-05, 2.8013e-06, 2.7883e-04,\n",
       "             7.4923e-05, 4.4897e-08, 2.9940e-04, 2.9105e-04, 5.0534e-04, 1.6482e-04,\n",
       "             1.0839e-06, 2.6791e-04, 2.1098e-04, 1.7795e-04, 1.1754e-04]],\n",
       "           device='cuda:0')},\n",
       "   1: {'step': 287,\n",
       "    'square_avg': tensor([9.7419e-05, 1.0180e-04, 3.4874e-05, 2.1513e-04, 5.7174e-05, 1.4421e-04,\n",
       "            2.4267e-04, 5.7302e-06, 1.0701e-04, 4.8444e-05, 2.0023e-04, 8.4680e-05,\n",
       "            5.9065e-04, 2.7467e-04, 3.6403e-04, 1.4765e-04, 5.3834e-05, 1.8979e-04,\n",
       "            2.0962e-04, 6.1583e-05, 1.6211e-04, 8.2656e-05, 3.5807e-04, 4.6898e-04,\n",
       "            4.7464e-05, 2.4105e-04], device='cuda:0')},\n",
       "   2: {'step': 287,\n",
       "    'square_avg': tensor([[1.8767e-05, 3.8366e-05, 3.3633e-05, 2.6461e-05, 1.7902e-05, 1.3064e-05,\n",
       "             3.6850e-05, 1.3206e-05, 4.4381e-05, 9.4735e-06, 1.2750e-05, 2.3025e-05,\n",
       "             1.5974e-05, 2.7946e-05, 1.5727e-05, 2.0091e-05, 2.2318e-05, 2.4192e-05,\n",
       "             1.7678e-05, 1.7062e-05, 2.4944e-05, 8.3806e-06, 1.6767e-05, 1.7373e-05,\n",
       "             1.2228e-05, 2.0771e-05],\n",
       "            [1.6329e-05, 4.0016e-05, 1.1725e-05, 1.8195e-05, 1.9848e-05, 1.2421e-05,\n",
       "             3.2250e-05, 6.9967e-06, 6.7981e-06, 1.5501e-06, 1.9099e-06, 6.4989e-06,\n",
       "             2.0443e-06, 2.0782e-05, 1.8656e-06, 3.3946e-06, 4.9819e-06, 5.2481e-06,\n",
       "             6.0969e-06, 1.5865e-05, 1.8091e-05, 1.9766e-06, 1.2158e-06, 1.6165e-06,\n",
       "             3.3676e-06, 1.2769e-05],\n",
       "            [4.2880e-05, 1.0870e-04, 3.0439e-05, 4.6452e-05, 5.4310e-05, 3.3036e-05,\n",
       "             8.7030e-05, 1.7725e-05, 1.6821e-05, 4.0151e-06, 4.7739e-06, 1.6713e-05,\n",
       "             6.4511e-06, 5.3601e-05, 5.4315e-06, 8.7589e-06, 1.2847e-05, 1.7131e-05,\n",
       "             1.4752e-05, 4.0947e-05, 4.4035e-05, 5.4931e-06, 4.7441e-06, 5.8277e-06,\n",
       "             8.5849e-06, 3.2379e-05],\n",
       "            [1.8327e-05, 4.5561e-05, 1.4819e-05, 2.3167e-05, 2.2842e-05, 1.5092e-05,\n",
       "             4.1590e-05, 7.4817e-06, 7.0730e-06, 1.4642e-06, 2.2212e-06, 8.0371e-06,\n",
       "             1.7101e-06, 2.5673e-05, 2.0726e-06, 3.8211e-06, 5.9476e-06, 3.7711e-06,\n",
       "             8.0920e-06, 2.0779e-05, 2.2745e-05, 1.4973e-06, 1.3881e-06, 1.5162e-06,\n",
       "             3.9031e-06, 1.6090e-05],\n",
       "            [2.3583e-05, 5.6812e-05, 1.7514e-05, 2.6446e-05, 2.8889e-05, 1.8077e-05,\n",
       "             4.7318e-05, 1.0046e-05, 1.0251e-05, 2.1494e-06, 2.6959e-06, 9.4489e-06,\n",
       "             3.1222e-06, 3.0577e-05, 2.4962e-06, 5.1377e-06, 7.6523e-06, 7.5598e-06,\n",
       "             9.0773e-06, 2.3367e-05, 2.6111e-05, 2.7996e-06, 2.2885e-06, 2.6938e-06,\n",
       "             4.8042e-06, 1.8643e-05],\n",
       "            [2.5782e-05, 6.5753e-05, 1.9358e-05, 2.8332e-05, 3.1424e-05, 1.9602e-05,\n",
       "             5.2878e-05, 1.1099e-05, 1.1100e-05, 2.5676e-06, 3.0914e-06, 1.0312e-05,\n",
       "             3.2977e-06, 3.3910e-05, 3.3341e-06, 5.1875e-06, 8.6797e-06, 9.3178e-06,\n",
       "             9.9216e-06, 2.5741e-05, 2.8810e-05, 3.1054e-06, 2.8937e-06, 3.9233e-06,\n",
       "             5.3709e-06, 2.0581e-05],\n",
       "            [1.3172e-05, 3.3066e-05, 9.3059e-06, 1.4312e-05, 1.6003e-05, 9.8841e-06,\n",
       "             2.6509e-05, 5.4904e-06, 5.3587e-06, 1.2231e-06, 1.3212e-06, 5.0832e-06,\n",
       "             1.6748e-06, 1.6977e-05, 1.8265e-06, 2.5807e-06, 4.2988e-06, 4.7585e-06,\n",
       "             5.1411e-06, 1.2939e-05, 1.4137e-05, 1.5846e-06, 1.7286e-06, 1.7730e-06,\n",
       "             2.7051e-06, 1.0323e-05],\n",
       "            [6.0676e-06, 9.2677e-06, 1.3677e-05, 9.7787e-06, 6.5233e-06, 5.0251e-06,\n",
       "             1.6173e-05, 4.9444e-06, 1.5031e-05, 3.6535e-06, 5.2060e-06, 9.2411e-06,\n",
       "             6.3064e-06, 9.0175e-06, 6.2481e-06, 7.1866e-06, 9.3376e-06, 9.1449e-06,\n",
       "             7.2295e-06, 7.4426e-06, 1.0555e-05, 3.1838e-06, 5.6300e-06, 6.9693e-06,\n",
       "             5.0458e-06, 9.0554e-06],\n",
       "            [1.4065e-05, 1.8668e-05, 2.9553e-05, 2.1994e-05, 1.4389e-05, 9.5165e-06,\n",
       "             2.3885e-05, 1.2961e-05, 3.1549e-05, 6.9972e-06, 1.0850e-05, 2.2056e-05,\n",
       "             1.1777e-05, 1.9942e-05, 1.2512e-05, 1.3856e-05, 1.6435e-05, 1.4271e-05,\n",
       "             1.5809e-05, 1.4044e-05, 2.2097e-05, 6.3180e-06, 1.1430e-05, 1.2908e-05,\n",
       "             1.3642e-05, 1.9301e-05],\n",
       "            [2.3187e-05, 3.5295e-05, 5.1058e-05, 2.9255e-05, 1.9740e-05, 1.4958e-05,\n",
       "             3.3556e-05, 1.8797e-05, 4.9090e-05, 1.3206e-05, 1.6005e-05, 3.4199e-05,\n",
       "             2.2053e-05, 2.8560e-05, 1.8808e-05, 2.7359e-05, 2.8900e-05, 2.5306e-05,\n",
       "             2.6407e-05, 2.2152e-05, 3.6966e-05, 1.1724e-05, 2.0389e-05, 2.1655e-05,\n",
       "             1.7829e-05, 2.7731e-05],\n",
       "            [4.2445e-06, 1.0733e-05, 3.0248e-06, 4.5165e-06, 4.9041e-06, 3.1806e-06,\n",
       "             8.6274e-06, 1.7463e-06, 1.7515e-06, 3.6659e-07, 4.4686e-07, 1.6114e-06,\n",
       "             5.6233e-07, 5.4808e-06, 4.5762e-07, 8.8194e-07, 1.3225e-06, 1.3459e-06,\n",
       "             1.5272e-06, 4.0324e-06, 4.6410e-06, 5.1710e-07, 3.8304e-07, 4.5007e-07,\n",
       "             8.0339e-07, 3.2499e-06],\n",
       "            [9.4065e-06, 2.1254e-05, 2.0654e-05, 1.9374e-05, 1.3403e-05, 9.7071e-06,\n",
       "             2.9691e-05, 9.6310e-06, 2.1210e-05, 6.2999e-06, 8.0950e-06, 1.8877e-05,\n",
       "             1.0713e-05, 1.6316e-05, 8.4838e-06, 1.4612e-05, 1.7339e-05, 1.6539e-05,\n",
       "             1.3412e-05, 1.4803e-05, 1.7609e-05, 5.8267e-06, 1.0758e-05, 1.1170e-05,\n",
       "             1.0410e-05, 1.3968e-05],\n",
       "            [1.2587e-05, 2.0471e-05, 2.9097e-05, 1.9559e-05, 1.1481e-05, 1.0265e-05,\n",
       "             2.7021e-05, 1.0593e-05, 3.0574e-05, 7.0157e-06, 1.0882e-05, 2.1368e-05,\n",
       "             1.1140e-05, 2.2048e-05, 1.1458e-05, 1.3766e-05, 1.6876e-05, 1.8164e-05,\n",
       "             1.3733e-05, 1.4650e-05, 1.9500e-05, 5.2041e-06, 1.0780e-05, 1.1574e-05,\n",
       "             1.0446e-05, 1.6413e-05],\n",
       "            [1.2443e-05, 3.0405e-05, 8.4595e-06, 1.3233e-05, 1.4687e-05, 8.9957e-06,\n",
       "             2.5630e-05, 4.8583e-06, 4.8308e-06, 9.2572e-07, 1.2730e-06, 4.4389e-06,\n",
       "             1.3578e-06, 1.6111e-05, 1.0846e-06, 2.3778e-06, 3.6048e-06, 3.1128e-06,\n",
       "             4.3808e-06, 1.1764e-05, 1.2991e-05, 1.4117e-06, 7.5964e-07, 1.1056e-06,\n",
       "             2.4638e-06, 9.4771e-06],\n",
       "            [4.2333e-09, 2.3315e-09, 1.0356e-10, 9.2668e-11, 2.4034e-09, 5.9674e-10,\n",
       "             3.7826e-11, 2.2296e-09, 4.8733e-09, 1.0097e-11, 2.0914e-10, 8.7931e-13,\n",
       "             1.2004e-09, 2.0746e-10, 2.6696e-13, 3.8676e-10, 2.4659e-11, 1.1727e-10,\n",
       "             7.9357e-10, 2.9663e-11, 3.4926e-12, 2.4241e-09, 2.6313e-11, 4.8296e-10,\n",
       "             2.9734e-12, 4.1129e-11],\n",
       "            [4.2028e-05, 1.0309e-04, 3.1423e-05, 4.3254e-05, 4.8228e-05, 3.0149e-05,\n",
       "             8.9753e-05, 1.6970e-05, 1.8524e-05, 3.1873e-06, 4.0691e-06, 1.6442e-05,\n",
       "             5.2582e-06, 5.8637e-05, 4.6595e-06, 8.6848e-06, 1.3819e-05, 1.2182e-05,\n",
       "             1.6218e-05, 4.2674e-05, 4.6264e-05, 4.9189e-06, 4.2340e-06, 4.6422e-06,\n",
       "             7.0827e-06, 3.3902e-05],\n",
       "            [7.8585e-06, 2.0785e-05, 4.9397e-06, 8.3228e-06, 9.0977e-06, 5.6648e-06,\n",
       "             1.5758e-05, 2.9542e-06, 3.0391e-06, 5.6592e-07, 7.6557e-07, 2.6783e-06,\n",
       "             7.9373e-07, 9.7234e-06, 7.5485e-07, 1.4418e-06, 2.3066e-06, 2.1705e-06,\n",
       "             2.6540e-06, 6.8479e-06, 8.0056e-06, 8.6472e-07, 5.3858e-07, 6.0097e-07,\n",
       "             1.4754e-06, 5.6104e-06],\n",
       "            [1.4881e-07, 2.6644e-07, 9.6628e-08, 4.3823e-08, 3.2843e-08, 3.0620e-08,\n",
       "             2.0960e-07, 2.5216e-08, 6.5245e-08, 1.0398e-08, 1.5592e-08, 5.9064e-09,\n",
       "             1.0070e-08, 1.2032e-07, 1.2120e-08, 8.2785e-09, 2.4356e-08, 1.0446e-08,\n",
       "             5.7640e-08, 5.4836e-08, 1.6120e-07, 2.1453e-09, 9.7881e-09, 3.8042e-09,\n",
       "             1.4935e-08, 1.1641e-07],\n",
       "            [1.2564e-05, 3.2005e-05, 1.1279e-05, 1.7823e-05, 1.8321e-05, 1.1270e-05,\n",
       "             2.8023e-05, 5.8180e-06, 5.1532e-06, 1.2992e-06, 1.7296e-06, 6.0101e-06,\n",
       "             1.7005e-06, 1.9155e-05, 1.5962e-06, 3.0732e-06, 5.0923e-06, 3.0121e-06,\n",
       "             5.9723e-06, 1.5410e-05, 1.6005e-05, 1.2320e-06, 1.3288e-06, 1.5977e-06,\n",
       "             3.2123e-06, 1.1835e-05],\n",
       "            [1.0373e-05, 2.3281e-05, 9.1010e-06, 1.3462e-05, 1.4115e-05, 8.8029e-06,\n",
       "             2.3740e-05, 4.5512e-06, 5.5035e-06, 8.5934e-07, 1.3153e-06, 4.7361e-06,\n",
       "             1.1939e-06, 1.3720e-05, 1.2117e-06, 2.4008e-06, 3.8752e-06, 2.0165e-06,\n",
       "             5.0090e-06, 1.2035e-05, 1.3325e-05, 1.2314e-06, 6.8430e-07, 1.0204e-06,\n",
       "             2.8237e-06, 9.4011e-06],\n",
       "            [7.2044e-06, 1.7252e-05, 5.3816e-06, 8.4510e-06, 9.1369e-06, 5.8748e-06,\n",
       "             1.5312e-05, 3.0373e-06, 3.1051e-06, 6.2128e-07, 5.0330e-07, 2.7788e-06,\n",
       "             8.5841e-07, 9.7750e-06, 7.9650e-07, 1.5289e-06, 2.4297e-06, 2.0183e-06,\n",
       "             2.8705e-06, 7.5198e-06, 8.6706e-06, 8.3218e-07, 6.6821e-07, 7.6682e-07,\n",
       "             1.1908e-06, 6.0680e-06],\n",
       "            [1.2360e-10, 2.0552e-10, 4.2178e-09, 1.6697e-09, 6.4848e-10, 1.9820e-09,\n",
       "             2.7569e-10, 4.3522e-09, 4.6270e-09, 5.4133e-09, 1.4790e-09, 3.5409e-10,\n",
       "             4.7965e-10, 1.3749e-10, 5.1130e-11, 1.6054e-08, 4.9194e-10, 5.2554e-10,\n",
       "             6.2604e-10, 6.7495e-11, 1.5668e-09, 2.6989e-10, 5.5372e-09, 8.0755e-10,\n",
       "             4.0965e-09, 3.4639e-10],\n",
       "            [9.8310e-06, 1.6352e-05, 2.0158e-05, 1.3399e-05, 9.1810e-06, 8.7507e-06,\n",
       "             1.9179e-05, 1.0163e-05, 1.8657e-05, 5.3446e-06, 6.4476e-06, 1.5174e-05,\n",
       "             9.2610e-06, 1.0786e-05, 6.9630e-06, 8.9970e-06, 1.0398e-05, 1.2318e-05,\n",
       "             8.1448e-06, 9.9640e-06, 1.4381e-05, 4.4773e-06, 7.7629e-06, 8.7100e-06,\n",
       "             8.8197e-06, 1.2187e-05],\n",
       "            [5.4414e-08, 7.2244e-08, 5.7732e-08, 5.1966e-08, 5.6000e-08, 4.1226e-08,\n",
       "             9.9831e-08, 3.2902e-08, 3.1273e-08, 1.0889e-08, 1.2562e-08, 2.9109e-08,\n",
       "             1.1399e-08, 6.7664e-08, 7.9601e-09, 2.3572e-08, 2.1936e-08, 1.6192e-08,\n",
       "             2.6388e-08, 7.1072e-08, 7.6164e-08, 1.4520e-08, 1.6495e-08, 9.9609e-09,\n",
       "             1.5640e-08, 5.3332e-08],\n",
       "            [1.5562e-05, 4.1908e-05, 1.3023e-05, 1.5982e-05, 1.6875e-05, 1.2481e-05,\n",
       "             3.7342e-05, 6.4002e-06, 4.9368e-06, 1.2852e-06, 1.3002e-06, 6.8672e-06,\n",
       "             2.0344e-06, 2.2125e-05, 1.7648e-06, 3.2338e-06, 5.6903e-06, 4.2616e-06,\n",
       "             6.6620e-06, 1.7367e-05, 2.0712e-05, 1.3176e-06, 1.4612e-06, 1.9262e-06,\n",
       "             2.0110e-06, 1.4249e-05],\n",
       "            [4.5276e-05, 1.0422e-04, 3.7382e-05, 5.3921e-05, 5.9013e-05, 2.7660e-05,\n",
       "             9.8825e-05, 2.2254e-05, 3.4707e-05, 8.4598e-06, 9.6535e-06, 2.8307e-05,\n",
       "             1.3414e-05, 4.5265e-05, 1.1230e-05, 1.7770e-05, 2.0359e-05, 2.9946e-05,\n",
       "             1.6077e-05, 2.4815e-05, 4.3913e-05, 1.1281e-05, 1.1418e-05, 1.2368e-05,\n",
       "             1.8142e-05, 2.6000e-05]], device='cuda:0')},\n",
       "   3: {'step': 287,\n",
       "    'square_avg': tensor([8.1135e-05, 3.0938e-05, 8.0795e-05, 3.6300e-05, 4.4412e-05, 5.0232e-05,\n",
       "            2.5493e-05, 3.2250e-05, 5.9846e-05, 1.0141e-04, 8.3095e-06, 5.9432e-05,\n",
       "            6.2383e-05, 2.3162e-05, 9.8694e-10, 7.9362e-05, 1.3671e-05, 1.2361e-07,\n",
       "            2.7428e-05, 2.0100e-05, 1.4208e-05, 1.0573e-08, 4.3631e-05, 1.1715e-07,\n",
       "            3.2216e-05, 1.0865e-04], device='cuda:0')},\n",
       "   4: {'step': 287,\n",
       "    'square_avg': tensor([[2.7955e-04, 2.5559e-04, 4.1885e-04, 3.9323e-04, 9.6698e-04, 1.3633e-03,\n",
       "             4.5086e-04, 1.4625e-04, 1.4483e-04, 1.5096e-04, 8.3748e-04, 1.5229e-04,\n",
       "             2.9394e-04, 6.7372e-04, 3.7692e-08, 5.2024e-04, 8.0863e-04, 2.8814e-06,\n",
       "             5.1729e-04, 1.1397e-04, 2.1351e-04, 2.4631e-09, 3.3809e-04, 1.2633e-04,\n",
       "             4.8079e-04, 2.6015e-04]], device='cuda:0')},\n",
       "   5: {'step': 287, 'square_avg': tensor([0.0010], device='cuda:0')}},\n",
       "  'param_groups': [{'lr': 0.0020337151702831666,\n",
       "    'momentum': 0,\n",
       "    'alpha': 0.99,\n",
       "    'eps': 1e-08,\n",
       "    'centered': False,\n",
       "    'weight_decay': 1.3130280280658598e-07,\n",
       "    'foreach': None,\n",
       "    'maximize': False,\n",
       "    'differentiable': False,\n",
       "    'params': [0, 1, 2, 3, 4, 5]}]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"data/support/models/best_ckpt__ode__support__rec_mlp__0__split_1__seed_1.pt\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf5ec0-0ee8-402e-8582-2ea3633b25ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
